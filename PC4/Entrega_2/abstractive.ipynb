{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T04:27:25.693498Z",
     "start_time": "2024-12-06T04:27:25.681360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")   \n"
   ],
   "id": "288258af4f79c1ff",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T04:27:25.708966Z",
     "start_time": "2024-12-06T04:27:25.694444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class FineTuneConfig:\n",
    "    \"\"\"\n",
    "    Configuración para el fine-tuning del modelo.\n",
    "    \"\"\"\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "    max_source_length: int = 1024\n",
    "    max_target_length: int = 128\n",
    "    batch_size: int = 4\n",
    "    num_train_epochs: int = 1\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    eval_strategy: str = \"epoch\"\n",
    "    predict_with_generate: bool = True\n",
    "    num_beams: int = 4\n"
   ],
   "id": "6e59b44438d55b54",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T04:27:25.724732Z",
     "start_time": "2024-12-06T04:27:25.709966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationPreprocessor:\n",
    "    \"\"\"\n",
    "    Clase para preprocesar el dataset CNN/DailyMail para tareas de resumen.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_source_length: int = 1024, max_target_length: int = 128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def preprocess_batch(self, batch: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Preprocesa un batch de datos del dataset CNN/DailyMail.\n",
    "\n",
    "        Args:\n",
    "            batch (Dict[str, List[str]]): Batch con claves \"article\" y \"highlights\".\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Diccionario con entradas tokenizadas y etiquetas.\n",
    "        \"\"\"\n",
    "        inputs = batch[\"article\"]\n",
    "        targets = batch[\"highlights\"]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs, \n",
    "            max_length=self.max_source_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Convertir las etiquetas eficientemente a tensores\n",
    "        labels = self.tokenizer(\n",
    "            targets, \n",
    "            max_length=self.max_target_length, \n",
    "            truncation=True, \n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"np\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        model_inputs[\"labels\"] = torch.tensor(np.array(labels), dtype=torch.int64)\n",
    "        return model_inputs\n"
   ],
   "id": "a2584289549c9d78",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T04:27:25.739963Z",
     "start_time": "2024-12-06T04:27:25.726732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationModel:\n",
    "    \"\"\"\n",
    "    Clase para encapsular la carga del modelo y el tokenizer,\n",
    "    facilitando la inicialización de diferentes LLM (T5, BART, PEGASUS).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name (str): Nombre del modelo en Hugging Face Hub.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    def get_components(self) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Retorna el tokenizer y el modelo.\n",
    "\n",
    "        Returns:\n",
    "            (tokenizer, model)\n",
    "        \"\"\"\n",
    "        return self.tokenizer, self.model\n"
   ],
   "id": "1095956df0831ed4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T04:27:25.755035Z",
     "start_time": "2024-12-06T04:27:25.740960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationTrainer:\n",
    "    \"\"\"\n",
    "    Clase responsable de entrenar, evaluar y guardar el modelo de resumen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            config: FineTuneConfig,\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            metric_name: str = \"rouge\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa el entrenador para el modelo de resumen.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Nombre del modelo en Hugging Face Hub.\n",
    "            config (FineTuneConfig): Configuración de fine-tuning.\n",
    "            train_dataset: Dataset de entrenamiento tokenizado.\n",
    "            val_dataset: Dataset de validación tokenizado.\n",
    "            metric_name (str): Nombre de la métrica a cargar (por defecto, 'rouge').\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.metric = evaluate.load(metric_name)\n",
    "        self.tokenizer, self.model = SummarizationModel(model_name).get_components()\n",
    "\n",
    "        self.data_collator = DataCollatorForSeq2Seq(\n",
    "            self.tokenizer, model=self.model, padding=\"longest\"\n",
    "        )\n",
    "\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            eval_strategy=self.config.eval_strategy,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            overwrite_output_dir=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \"\"\"\n",
    "        Computa métricas a partir de las predicciones del modelo.\n",
    "\n",
    "        Args:\n",
    "            eval_preds: Tupla (predictions, labels) del conjunto de validación.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Métricas calculadas (ROUGE).\n",
    "        \"\"\"\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = self.tokenizer.batch_decode(\n",
    "            preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        decoded_labels = self.tokenizer.batch_decode(\n",
    "            labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        # ROUGE eval\n",
    "        result = self.metric.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        # Promediar y redondear\n",
    "        result = {k: round(v.mid.fmeasure * 100, 2) for k, v in result.items()}\n",
    "        return result\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        \"\"\"\n",
    "        Entrena el modelo y lo evalúa en el conjunto de validación,\n",
    "        luego guarda el modelo fine-tuneado.\n",
    "        \"\"\"\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Evaluation results for {self.model_name}: {eval_results}\")\n",
    "\n",
    "        # Guardar el modelo fine-tuneado\n",
    "        trainer.save_model(self.config.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.config.output_dir)\n",
    "        print(f\"Modelo guardado en: {self.config.output_dir}\")\n"
   ],
   "id": "fb30b2e449884c1c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T04:27:25.770563Z",
     "start_time": "2024-12-06T04:27:25.755035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta el pipeline de:\n",
    "    1. Carga y preprocesamiento del dataset CNN/DailyMail.\n",
    "    2. Entrenamiento y evaluación de T5, BART y PEGASUS.\n",
    "    3. Guardado de los modelos fine-tuneados.\n",
    "    \"\"\"\n",
    "    # Carga el dataset\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    train_dataset_raw = dataset[\"train\"]\n",
    "    val_dataset_raw = dataset[\"validation\"]\n",
    "\n",
    "    # Configuraciones de cada modelo\n",
    "    models_to_train = [\n",
    "        {\n",
    "            \"model_name\": \"t5-small\",\n",
    "            \"output_dir\": \"resources/t5_finetuned_cnn\",\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"facebook/bart-large-cnn\",\n",
    "            \"output_dir\": \"resources/bart_finetuned_cnn\",\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"google/pegasus-cnn_dailymail\",\n",
    "            \"output_dir\": \"resources/pegasus_finetuned_cnn\",\n",
    "        }\n",
    "    ]\n",
    "    # t5-small | google-t5/t5-large\n",
    "    # facebook/bart-large-cnn | facebook/bart-large\n",
    "    # google/pegasus-cnn_dailymail | google/pegasus-large\n",
    "\n",
    "    for model_info in models_to_train:\n",
    "        model_name = model_info[\"model_name\"]\n",
    "        output_dir = model_info[\"output_dir\"]\n",
    "\n",
    "        # Inicializar preprocesador\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        preprocessor = SummarizationPreprocessor(\n",
    "            tokenizer=tokenizer,\n",
    "            max_source_length=1024,\n",
    "            max_target_length=128\n",
    "        )\n",
    "\n",
    "        # Tokenizar datasets\n",
    "        tokenized_train = train_dataset_raw.map(\n",
    "            preprocessor.preprocess_batch,\n",
    "            batched=True,\n",
    "            remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    "        )\n",
    "        tokenized_val = val_dataset_raw.map(\n",
    "            preprocessor.preprocess_batch,\n",
    "            batched=True,\n",
    "            remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    "        )\n",
    "\n",
    "        tokenized_train.set_format(\"torch\")\n",
    "        tokenized_val.set_format(\"torch\")\n",
    "\n",
    "        # Crear config de fine-tuning\n",
    "        config = FineTuneConfig(\n",
    "            model_name=model_name,\n",
    "            output_dir=output_dir,\n",
    "            max_source_length=1024,\n",
    "            max_target_length=128,\n",
    "            batch_size=10,\n",
    "            num_train_epochs=100,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            predict_with_generate=True,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "        # Entrenar y evaluar el modelo\n",
    "        trainer = SummarizationTrainer(\n",
    "            model_name=model_name,\n",
    "            config=config,\n",
    "            train_dataset=tokenized_train,\n",
    "            val_dataset=tokenized_val\n",
    "        )\n",
    "        trainer.train_and_evaluate()\n"
   ],
   "id": "6705c51901ad9773",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-06T04:27:25.771565Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "50d119863a3a7c10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57deada39abb4633a46fcfe9741fdddd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
