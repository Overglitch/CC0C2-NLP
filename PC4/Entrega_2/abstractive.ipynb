{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:22.109740Z",
     "start_time": "2024-12-10T23:28:15.676943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Tuple\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n"
   ],
   "id": "288258af4f79c1ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Overglitch\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:22.124610Z",
     "start_time": "2024-12-10T23:28:22.110746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class FineTuneConfig:\n",
    "    \"\"\"\n",
    "    Configuración para el fine-tuning del modelo.\n",
    "    \"\"\"\n",
    "    model_name: str\n",
    "    output_dir: str\n",
    "    save_strategy: str\n",
    "    max_source_length: int = 256\n",
    "    max_target_length: int = 32\n",
    "    batch_size: int = 8\n",
    "    num_train_epochs: int = 100\n",
    "    learning_rate: float = 2e-5\n",
    "    weight_decay: float = 0.01\n",
    "    eval_strategy: str = \"epoch\"\n",
    "    predict_with_generate: bool = True\n",
    "    num_beams: int = 8\n"
   ],
   "id": "6e59b44438d55b54",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:22.139566Z",
     "start_time": "2024-12-10T23:28:22.125611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationPreprocessor:\n",
    "    \"\"\"\n",
    "    Clase para preprocesar el dataset CNN/DailyMail para tareas de resumen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, max_source_length: int = 256, max_target_length: int = 32):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def preprocess_batch(self, batch: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Preprocesa un batch de datos del dataset CNN/DailyMail.\n",
    "    \n",
    "        Args:\n",
    "            batch (Dict[str, List[str]]): Batch con claves \"article\" y \"highlights\".\n",
    "    \n",
    "        Returns:\n",
    "            Dict[str, Any]: Diccionario con entradas tokenizadas y etiquetas.\n",
    "        \"\"\"\n",
    "        inputs = batch[\"article\"]\n",
    "        targets = batch[\"highlights\"]\n",
    "\n",
    "        # Tokenizar inputs\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_source_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"  # PyTorch directamente\n",
    "        )\n",
    "\n",
    "        # Tokenizar etiquetas\n",
    "        labels = self.tokenizer(\n",
    "            targets,\n",
    "            max_length=self.max_target_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Convertir lista de listas a un único array NumPy\n",
    "        labels_array = np.array(labels, dtype=np.int64)\n",
    "\n",
    "        # Convertir el array NumPy a tensor de PyTorch\n",
    "        model_inputs[\"labels\"] = torch.tensor(labels_array, dtype=torch.int64)\n",
    "\n",
    "        return model_inputs\n"
   ],
   "id": "a2584289549c9d78",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:22.155582Z",
     "start_time": "2024-12-10T23:28:22.140567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationModel:\n",
    "    \"\"\"\n",
    "    Clase para encapsular la carga del modelo y el tokenizer,\n",
    "    facilitando la inicialización de diferentes LLM (T5, BART, PEGASUS).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name (str): Nombre del modelo en Hugging Face Hub.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    def get_components(self) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Retorna el tokenizer y el modelo.\n",
    "\n",
    "        Returns:\n",
    "            (tokenizer, model)\n",
    "        \"\"\"\n",
    "        return self.tokenizer, self.model\n"
   ],
   "id": "1095956df0831ed4",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:22.171464Z",
     "start_time": "2024-12-10T23:28:22.156582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SummarizationTrainer:\n",
    "    \"\"\"\n",
    "    Clase responsable de entrenar, evaluar y guardar el modelo de resumen.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model_name: str,\n",
    "            config: FineTuneConfig,\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            metric_name: str = \"rouge\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa el entrenador para el modelo de resumen.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): Nombre del modelo en Hugging Face Hub.\n",
    "            config (FineTuneConfig): Configuración de fine-tuning.\n",
    "            train_dataset: Dataset de entrenamiento tokenizado.\n",
    "            val_dataset: Dataset de validación tokenizado.\n",
    "            metric_name (str): Nombre de la métrica a cargar (por defecto, 'rouge').\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.config = config\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.metric = evaluate.load(metric_name)\n",
    "        self.tokenizer, self.model = SummarizationModel(model_name).get_components()\n",
    "\n",
    "        self.data_collator = DataCollatorForSeq2Seq(\n",
    "            self.tokenizer, model=self.model, padding=\"longest\"\n",
    "        )\n",
    "\n",
    "        self.training_args = TrainingArguments(\n",
    "            output_dir=self.config.output_dir,\n",
    "            eval_strategy=self.config.eval_strategy,\n",
    "            save_strategy=self.config.save_strategy,\n",
    "            per_device_train_batch_size=self.config.batch_size,\n",
    "            per_device_eval_batch_size=self.config.batch_size,\n",
    "            learning_rate=self.config.learning_rate,\n",
    "            weight_decay=self.config.weight_decay,\n",
    "            num_train_epochs=self.config.num_train_epochs,\n",
    "            overwrite_output_dir=True\n",
    "        )\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \"\"\"\n",
    "        Computa métricas a partir de las predicciones del modelo.\n",
    "\n",
    "        Args:\n",
    "            eval_preds: Tupla (predictions, labels) del conjunto de validación.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Métricas calculadas (ROUGE).\n",
    "        \"\"\"\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = self.tokenizer.batch_decode(\n",
    "            preds, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        decoded_labels = self.tokenizer.batch_decode(\n",
    "            labels, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
    "        )\n",
    "\n",
    "        # ROUGE eval\n",
    "        result = self.metric.compute(\n",
    "            predictions=decoded_preds,\n",
    "            references=decoded_labels,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        # Promediar y redondear\n",
    "        result = {k: round(v.mid.fmeasure * 100, 2) for k, v in result.items()}\n",
    "        return result\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        \"\"\"\n",
    "        Entrena el modelo y lo evalúa en el conjunto de validación,\n",
    "        luego guarda el modelo fine-tuneado.\n",
    "        \"\"\"\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=self.training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=self.data_collator,\n",
    "            compute_metrics=self.compute_metrics\n",
    "        )\n",
    "        trainer.train()\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Evaluation results for {self.model_name}: {eval_results}\")\n",
    "\n",
    "        # Guardar el modelo fine-tuneado\n",
    "        trainer.save_model(self.config.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.config.output_dir)\n",
    "        print(f\"Modelo guardado en: {self.config.output_dir}\")\n"
   ],
   "id": "fb30b2e449884c1c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:28:22.187090Z",
     "start_time": "2024-12-10T23:28:22.172465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta el pipeline de:\n",
    "    1. Carga y preprocesamiento del dataset CNN/DailyMail.\n",
    "    2. Entrenamiento y evaluación de T5, BART y PEGASUS.\n",
    "    3. Guardado de los modelos fine-tuneados.\n",
    "    \"\"\"\n",
    "    # Carga el dataset\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "    train_dataset_raw = dataset[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "    val_dataset_raw = dataset[\"validation\"].shuffle(seed=42).select(range(200))\n",
    "    test_dataset_raw = dataset[\"test\"].shuffle(seed=42).select(range(200))\n",
    "\n",
    "    # Configuraciones de cada modelo\n",
    "    models_to_train = [\n",
    "        {\n",
    "            \"model_name\": \"t5-small\",\n",
    "            \"output_dir\": \"resources/t5_finetuned_cnn\",\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"facebook/bart-large-cnn\",\n",
    "            \"output_dir\": \"resources/bart_finetuned_cnn\",\n",
    "        },\n",
    "        {\n",
    "            \"model_name\": \"google/pegasus-cnn_dailymail\",\n",
    "            \"output_dir\": \"resources/pegasus_finetuned_cnn\",\n",
    "        }\n",
    "    ]\n",
    "    # t5-small | google-t5/t5-large\n",
    "    # facebook/bart-large-cnn | facebook/bart-large\n",
    "    # google/pegasus-cnn_dailymail | google/pegasus-large\n",
    "\n",
    "    for model_info in models_to_train:\n",
    "        model_name = model_info[\"model_name\"]\n",
    "        output_dir = model_info[\"output_dir\"]\n",
    "\n",
    "        # Inicializar preprocesador\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "        preprocessor = SummarizationPreprocessor(\n",
    "            tokenizer=tokenizer,\n",
    "            max_source_length=256,\n",
    "            max_target_length=32\n",
    "        )\n",
    "\n",
    "        # Tokenizar datasets\n",
    "        tokenized_train = train_dataset_raw.map(\n",
    "            preprocessor.preprocess_batch,\n",
    "            batched=True,\n",
    "            remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    "        )\n",
    "        tokenized_val = val_dataset_raw.map(\n",
    "            preprocessor.preprocess_batch,\n",
    "            batched=True,\n",
    "            remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    "        )\n",
    "\n",
    "        tokenized_train.set_format(\"torch\")\n",
    "        tokenized_val.set_format(\"torch\")\n",
    "\n",
    "        # Crear config de fine-tuning\n",
    "        config = FineTuneConfig(\n",
    "            model_name=model_name,\n",
    "            output_dir=output_dir,\n",
    "            save_strategy=\"epoch\",\n",
    "            max_source_length=256,\n",
    "            max_target_length=32,\n",
    "            batch_size=4,\n",
    "            num_train_epochs=1,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            predict_with_generate=False,\n",
    "            num_beams=4\n",
    "        )\n",
    "\n",
    "        # Entrenar y evaluar el modelo\n",
    "        trainer = SummarizationTrainer(\n",
    "            model_name=model_name,\n",
    "            config=config,\n",
    "            train_dataset=tokenized_train,\n",
    "            val_dataset=tokenized_val\n",
    "        )\n",
    "        trainer.train_and_evaluate()\n"
   ],
   "id": "6705c51901ad9773",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T23:29:19.931170Z",
     "start_time": "2024-12-10T23:28:22.188090Z"
    }
   },
   "cell_type": "code",
   "source": "main()",
   "id": "724a8c3c6115e609",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bf089aa752744108c531a44abd67d34"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "37fd449989f64ff4ae0b471643204fd7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Overglitch\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\data\\data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='251' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:15]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[6], line 83\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# Entrenar y evaluar el modelo\u001B[39;00m\n\u001B[0;32m     77\u001B[0m trainer \u001B[38;5;241m=\u001B[39m SummarizationTrainer(\n\u001B[0;32m     78\u001B[0m     model_name\u001B[38;5;241m=\u001B[39mmodel_name,\n\u001B[0;32m     79\u001B[0m     config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[0;32m     80\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mtokenized_train,\n\u001B[0;32m     81\u001B[0m     val_dataset\u001B[38;5;241m=\u001B[39mtokenized_val\n\u001B[0;32m     82\u001B[0m )\n\u001B[1;32m---> 83\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[5], line 91\u001B[0m, in \u001B[0;36mSummarizationTrainer.train_and_evaluate\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;124;03mEntrena el modelo y lo evalúa en el conjunto de validación,\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03mluego guarda el modelo fine-tuneado.\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     82\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m     83\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[0;32m     84\u001B[0m     args\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     89\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics\n\u001B[0;32m     90\u001B[0m )\n\u001B[1;32m---> 91\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     92\u001B[0m eval_results \u001B[38;5;241m=\u001B[39m trainer\u001B[38;5;241m.\u001B[39mevaluate()\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEvaluation results for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00meval_results\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:2052\u001B[0m, in \u001B[0;36mTrainer.train\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   2050\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[0;32m   2051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2053\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2054\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2055\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2056\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2057\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:2487\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2484\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_training_stop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   2486\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_epoch_end(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m-> 2487\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_maybe_log_save_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtr_loss\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_norm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2489\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m DebugOption\u001B[38;5;241m.\u001B[39mTPU_METRICS_DEBUG \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdebug:\n\u001B[0;32m   2490\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_torch_xla_available():\n\u001B[0;32m   2491\u001B[0m         \u001B[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:2915\u001B[0m, in \u001B[0;36mTrainer._maybe_log_save_evaluate\u001B[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   2913\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   2914\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_evaluate:\n\u001B[1;32m-> 2915\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2917\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol\u001B[38;5;241m.\u001B[39mshould_save:\n\u001B[0;32m   2918\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_save_checkpoint(model, trial, metrics\u001B[38;5;241m=\u001B[39mmetrics)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:2872\u001B[0m, in \u001B[0;36mTrainer._evaluate\u001B[1;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001B[0m\n\u001B[0;32m   2871\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_evaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, trial, ignore_keys_for_eval, skip_scheduler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m-> 2872\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2873\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_report_to_hp_search(trial, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step, metrics)\n\u001B[0;32m   2875\u001B[0m     \u001B[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:3868\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   3865\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m   3867\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[1;32m-> 3868\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3869\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3870\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3871\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[0;32m   3872\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[0;32m   3873\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   3874\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3875\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3876\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3878\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[0;32m   3879\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\trainer.py:4160\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[0;32m   4156\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_metrics(\n\u001B[0;32m   4157\u001B[0m             EvalPrediction(predictions\u001B[38;5;241m=\u001B[39mall_preds, label_ids\u001B[38;5;241m=\u001B[39mall_labels, inputs\u001B[38;5;241m=\u001B[39mall_inputs)\n\u001B[0;32m   4158\u001B[0m         )\n\u001B[0;32m   4159\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 4160\u001B[0m         metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mEvalPrediction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredictions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_preds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mall_labels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4161\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m metrics \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4162\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m {}\n",
      "Cell \u001B[1;32mIn[5], line 60\u001B[0m, in \u001B[0;36mSummarizationTrainer.compute_metrics\u001B[1;34m(self, eval_preds)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(preds, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m     59\u001B[0m     preds \u001B[38;5;241m=\u001B[39m preds[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 60\u001B[0m decoded_preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m     62\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m decoded_labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(\n\u001B[0;32m     64\u001B[0m     labels, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     65\u001B[0m )\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# ROUGE eval\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3959\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.batch_decode\u001B[1;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m   3935\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_decode\u001B[39m(\n\u001B[0;32m   3936\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   3937\u001B[0m     sequences: Union[List[\u001B[38;5;28mint\u001B[39m], List[List[\u001B[38;5;28mint\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3940\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3941\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m   3942\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3943\u001B[0m \u001B[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001B[39;00m\n\u001B[0;32m   3944\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3957\u001B[0m \u001B[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001B[39;00m\n\u001B[0;32m   3958\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 3959\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m   3960\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(\n\u001B[0;32m   3961\u001B[0m             seq,\n\u001B[0;32m   3962\u001B[0m             skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[0;32m   3963\u001B[0m             clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[0;32m   3964\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3965\u001B[0m         )\n\u001B[0;32m   3966\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m sequences\n\u001B[0;32m   3967\u001B[0m     ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3960\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m   3935\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbatch_decode\u001B[39m(\n\u001B[0;32m   3936\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   3937\u001B[0m     sequences: Union[List[\u001B[38;5;28mint\u001B[39m], List[List[\u001B[38;5;28mint\u001B[39m]], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.ndarray\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtf.Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3940\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3941\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m   3942\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   3943\u001B[0m \u001B[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001B[39;00m\n\u001B[0;32m   3944\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   3957\u001B[0m \u001B[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001B[39;00m\n\u001B[0;32m   3958\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   3959\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m-> 3960\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode(\n\u001B[0;32m   3961\u001B[0m             seq,\n\u001B[0;32m   3962\u001B[0m             skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[0;32m   3963\u001B[0m             clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[0;32m   3964\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3965\u001B[0m         )\n\u001B[0;32m   3966\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m sequences\n\u001B[0;32m   3967\u001B[0m     ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3999\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m   3996\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[0;32m   3997\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[1;32m-> 3999\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode(\n\u001B[0;32m   4000\u001B[0m     token_ids\u001B[38;5;241m=\u001B[39mtoken_ids,\n\u001B[0;32m   4001\u001B[0m     skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[0;32m   4002\u001B[0m     clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[0;32m   4003\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4004\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:654\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(token_ids, \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m    653\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m [token_ids]\n\u001B[1;32m--> 654\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    656\u001B[0m clean_up_tokenization_spaces \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    657\u001B[0m     clean_up_tokenization_spaces\n\u001B[0;32m    658\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    659\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclean_up_tokenization_spaces\n\u001B[0;32m    660\u001B[0m )\n\u001B[0;32m    661\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces:\n",
      "\u001B[1;31mTypeError\u001B[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
