{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Librerías e inicialización",
   "id": "d9a078e99258a23c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T11:58:12.040544Z",
     "start_time": "2024-11-29T11:58:06.980667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importar las librerías necesarias\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import fitz  # PyMuPDF para lectura de PDF\n",
    "from typing import List\n",
    "from rouge_score import rouge_scorer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from summarizer import Summarizer  # bert-extractive-summarizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Descargar recursos necesarios para NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ],
   "id": "9bdcef06ab6ba438",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Clase: DocumentReader",
   "id": "23047116129189f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T11:58:12.056276Z",
     "start_time": "2024-11-29T11:58:12.041543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DocumentReader:\n",
    "    \"\"\"Clase para leer documentos PDF y extraer texto plano.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def read_document(self) -> str:\n",
    "        \"\"\"Lee y extrae texto de un archivo PDF usando PyMuPDF.\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with fitz.open(self.file_path) as doc:\n",
    "                for page in doc:\n",
    "                    text += page.get_text()  # Extrae texto de cada página\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error leyendo el archivo PDF: {e}\")\n",
    "        return text\n"
   ],
   "id": "54f3945468cd5f14",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Clase: Preprocessor",
   "id": "4bb6866bac75d582"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T11:58:12.071524Z",
     "start_time": "2024-11-29T11:58:12.057277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"Clase para preprocesar texto: limpieza, tokenización y filtrado.\"\"\"\n",
    "\n",
    "    def __init__(self, language: str = 'english'):\n",
    "        self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "        self.stemmer = SnowballStemmer(language)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_sentences(text: str) -> List[str]:\n",
    "        \"\"\"Tokeniza texto en oraciones.\"\"\"\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "    def preprocess_sentences(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Limpia y preprocesa oraciones eliminando ruido.\n",
    "        - Filtra oraciones con muchos números o palabras irrelevantes.\n",
    "        - Aplica minúsculas, stemming y lematización.\n",
    "        \"\"\"\n",
    "        preprocessed = []\n",
    "        for sentence in sentences:\n",
    "            # Filtra oraciones con alta densidad de números\n",
    "            if sum(char.isdigit() for char in sentence) / max(len(sentence), 1) > 0.3:\n",
    "                continue\n",
    "\n",
    "            # Conversión a minúsculas\n",
    "            sentence = sentence.lower()\n",
    "            # Eliminación de caracteres no alfanuméricos\n",
    "            sentence = re.sub(r'[^a-zA-Z0-9\\s]', '', sentence)\n",
    "            # Tokenización y procesamiento de palabras\n",
    "            words = word_tokenize(sentence)\n",
    "            words = [\n",
    "                self.lemmatizer.lemmatize(self.stemmer.stem(word))\n",
    "                for word in words if word not in self.stopwords\n",
    "            ]\n",
    "            if words:  # Excluir oraciones vacías\n",
    "                preprocessed.append(' '.join(words))\n",
    "        return preprocessed\n"
   ],
   "id": "aff549d080322b8d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Clases de Resumen (TF-IDF, TextRank, TF-IDF+TextRank, BERT)",
   "id": "37375831e9b873a6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T11:58:12.086739Z",
     "start_time": "2024-11-29T11:58:12.072528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TFIDFSummarizer:\n",
    "    \"\"\"Genera resúmenes usando el modelo TF-IDF.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def summarize(sentences: List[str], preprocessed_sentences: List[str], num_sentences: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Genera un resumen basado en TF-IDF seleccionando las oraciones mejor puntuadas.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
    "        sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)\n",
    "        ranked_indices = np.argsort(sentence_scores)[::-1]\n",
    "        selected = [sentences[i] for i in ranked_indices[:num_sentences]]\n",
    "        return ' '.join(selected)\n",
    "\n",
    "\n",
    "class TextRankSummarizer:\n",
    "    \"\"\"Genera resúmenes usando el algoritmo TextRank.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def summarize(sentences: List[str], preprocessed_sentences: List[str], num_sentences: int = 1) -> str:\n",
    "        \"\"\"\n",
    "        Genera un resumen usando el algoritmo de grafos TextRank.\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
    "        similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "        nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "        scores = nx.pagerank(nx_graph)\n",
    "        ranked_indices = sorted(((scores[node], node) for node in nx_graph.nodes), reverse=True)\n",
    "        selected = [sentences[i] for _, i in ranked_indices[:num_sentences]]\n",
    "        return ' '.join(selected)\n",
    "\n",
    "\n",
    "class CombinedSummarizer:\n",
    "    \"\"\"Genera resúmenes combinando palabras clave TF-IDF y TextRank.\"\"\"\n",
    "\n",
    "    def __init__(self, top_n_keywords: int = 10):\n",
    "        self.top_n_keywords = top_n_keywords\n",
    "\n",
    "    def extract_keywords_tfidf(self, preprocessed_sentences: List[str]) -> List[str]:\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
    "        tfidf_scores = zip(vectorizer.get_feature_names_out(), tfidf_matrix.toarray().sum(axis=0))\n",
    "        sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in sorted_scores[:self.top_n_keywords]]\n",
    "\n",
    "    def extract_keywords_textrank(self, preprocessed_sentences: List[str]) -> List[str]:\n",
    "        words = ' '.join(preprocessed_sentences).split()\n",
    "        co_occurrence_graph = nx.Graph()\n",
    "        for i in range(len(words) - 1):\n",
    "            word_pair = (words[i], words[i + 1])\n",
    "            if co_occurrence_graph.has_edge(*word_pair):\n",
    "                co_occurrence_graph[word_pair[0]][word_pair[1]]['weight'] += 1\n",
    "            else:\n",
    "                co_occurrence_graph.add_edge(word_pair[0], word_pair[1], weight=1)\n",
    "        ranks = nx.pagerank(co_occurrence_graph, weight='weight')\n",
    "        sorted_ranks = sorted(ranks.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [word for word, _ in sorted_ranks[:self.top_n_keywords]]\n",
    "\n",
    "    def combined_keywords(self, preprocessed_sentences: List[str]) -> List[str]:\n",
    "        tfidf_keywords = self.extract_keywords_tfidf(preprocessed_sentences)\n",
    "        textrank_keywords = self.extract_keywords_textrank(preprocessed_sentences)\n",
    "        return list(set(tfidf_keywords) & set(textrank_keywords))\n",
    "\n",
    "    def summarize(self, sentences: List[str], preprocessed_sentences: List[str], num_sentences: int = 1) -> str:\n",
    "        keywords = self.combined_keywords(preprocessed_sentences)\n",
    "        sentence_scores = []\n",
    "        for i, sentence in enumerate(preprocessed_sentences):\n",
    "            score = sum(1 for word in sentence.split() if word in keywords)\n",
    "            sentence_scores.append((score, i))\n",
    "        ranked_sentences = sorted(sentence_scores, key=lambda x: x[0], reverse=True)\n",
    "        selected = [sentences[i] for _, i in ranked_sentences[:num_sentences]]\n",
    "        return ' '.join(selected)\n",
    "\n",
    "\n",
    "class BERTSummarizer:\n",
    "    \"\"\"Genera resúmenes usando un modelo BERT extractivo preentrenado.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = Summarizer()\n",
    "\n",
    "    def summarize(self, text: str, num_sentences: int = 1) -> str:\n",
    "        return ''.join(self.model(text, num_sentences=num_sentences))\n"
   ],
   "id": "7055df590c83c9f9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Evaluación de ROUGE",
   "id": "b247965436c1b8a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T12:04:24.459309Z",
     "start_time": "2024-11-29T12:04:24.453799Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_rouge_scores(reference_summary: str, generated_summary: str) -> dict:\n",
    "    \"\"\"Calcula métricas ROUGE entre un resumen de referencia y el generado.\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference_summary, generated_summary)\n",
    "    return {\n",
    "        'ROUGE-1': scores['rouge1'].fmeasure,\n",
    "        'ROUGE-2': scores['rouge2'].fmeasure,\n",
    "        'ROUGE-L': scores['rougeL'].fmeasure\n",
    "    }\n"
   ],
   "id": "80068ae9aafb5c30",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Ejecución",
   "id": "36dfb8653ee74658"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T12:09:37.067466Z",
     "start_time": "2024-11-29T12:09:34.999865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parámetros iniciales\n",
    "file_path = 'resources/f5df9a1942d2e626c5448416a05ffc9ca2d2369c.txt'  # Ruta del archivo\n",
    "reference_summary = \"\"\"\n",
    "    Deputy police commissioner Nick Kaldas is giving evidence at an inquiry . Kaldas, 57, is a counter terrorism expert who has trained Iraqi police . He arrived in Australia aged 12 and fluent in English, French and Arabic . The inquiry is into a illegal police bugging operation of 114 people in 2000 . Kaldas is the highest ranking officer secretly bugged by his rival Kath Burn . He has 'explosive' evidence about bugging which has 'denigrated' his career . He has suffered reprisals for speaking out about the bugging scandal . The bugging operation threatens to blow apart NSW police hierarchy . He said independent inquiry into bugging scandal has left him fearful . Claimed Operation Prospect had sided with the officers being complained about and targeted him and other victims .\n",
    "    \"\"\"\n",
    "num_sentences = 1\n",
    "\n",
    "# Lectura y preprocesamiento\n",
    "reader = DocumentReader(file_path)\n",
    "text = reader.read_document()\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "sentences = preprocessor.tokenize_sentences(text)\n",
    "preprocessed_sentences = preprocessor.preprocess_sentences(sentences)\n",
    "\n",
    "# Generar resúmenes\n",
    "tfidf_summarizer = TFIDFSummarizer()\n",
    "tfidf_summary = tfidf_summarizer.summarize(sentences, preprocessed_sentences, num_sentences)\n",
    "\n",
    "textrank_summarizer = TextRankSummarizer()\n",
    "textrank_summary = textrank_summarizer.summarize(sentences, preprocessed_sentences, num_sentences)\n",
    "\n",
    "combined_summarizer = CombinedSummarizer()\n",
    "combined_summary = combined_summarizer.summarize(sentences, preprocessed_sentences, num_sentences)\n",
    "\n",
    "bert_summarizer = BERTSummarizer()\n",
    "bert_summary = bert_summarizer.summarize(text, num_sentences)\n",
    "\n",
    "# Imprimir resúmenes\n",
    "print(f\"Resumen TF-IDF:\\n{tfidf_summary}\\n\")\n",
    "print(f\"Resumen TextRank:\\n{textrank_summary}\\n\")\n",
    "print(f\"Resumen Combinado:\\n{combined_summary}\\n\")\n",
    "print(f\"Resumen BERT:\\n{bert_summary}\\n\")"
   ],
   "id": "2e9ef2d11a0511a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen TF-IDF:\n",
      "On the ground: Nick Kaldas, pictured with\n",
      "then police minister John Watkins in 2006, warned on\n",
      "his return from Iraq that year that 'Australia is\n",
      "very much part of the international community and if\n",
      "something happens in Palestine or Iraq, we have to\n",
      "accept that it has an impact over here' In the lead\n",
      "up to the inquiry, Ms Burn has denied falsifying\n",
      "evidence in the warrant to Judge Bell to secretly bug\n",
      "police, or to 'use illegal warrants to secretly\n",
      "record conversations of my rivals in the police\n",
      "force'.\n",
      "\n",
      "Resumen TextRank:\n",
      "Man of integrity:\n",
      "One of the country's most distinguished police\n",
      "officers, NSW deputy commissioner Nick Kaldas\n",
      "(pictured) has worked in Iraq, locked up murderers\n",
      "and trained with the FBI, but his stellar career has\n",
      "been dogged by an illegal bugging operation\n",
      "commandeered in 2000 by his rival for the top job .\n",
      "\n",
      "Resumen Combinado:\n",
      "NSW Deputy Police\n",
      "Commissioner Nick Kaldas told a parliamentary inquiry\n",
      "he has been punished for speaking out, including\n",
      "being passed over for promotion, while attacking the\n",
      "way the NSW Ombudsman has handled a two-year\n",
      "investigation into the bugging scandal that reaches\n",
      "to the highest levels of the NSW Police Force.\n",
      "\n",
      "Resumen BERT:\n",
      "A top policeman who was accused of whistle blowing\n",
      "after he had his home and office bugged by colleagues\n",
      "has told how he's been left him 'humiliated' by an\n",
      "investigation into the scandal.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Resultados",
   "id": "17881e938587bc86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T12:09:56.554173Z",
     "start_time": "2024-11-29T12:09:56.530325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluar con ROUGE\n",
    "tfidf_rouge = calculate_rouge_scores(reference_summary, tfidf_summary)\n",
    "textrank_rouge = calculate_rouge_scores(reference_summary, textrank_summary)\n",
    "combined_rouge = calculate_rouge_scores(reference_summary, combined_summary)\n",
    "bert_rouge = calculate_rouge_scores(reference_summary, bert_summary)\n",
    "\n",
    "# Crear tabla de resultados\n",
    "\n",
    "data = {\n",
    "    'Modelo': ['TF-IDF', 'TextRank', 'Combinado', 'BERT'],\n",
    "    'ROUGE-1': [tfidf_rouge['ROUGE-1'], textrank_rouge['ROUGE-1'], combined_rouge['ROUGE-1'],\n",
    "                bert_rouge['ROUGE-1']],\n",
    "    'ROUGE-2': [tfidf_rouge['ROUGE-2'], textrank_rouge['ROUGE-2'], combined_rouge['ROUGE-2'],\n",
    "                bert_rouge['ROUGE-2']],\n",
    "    'ROUGE-L': [tfidf_rouge['ROUGE-L'], textrank_rouge['ROUGE-L'], combined_rouge['ROUGE-L'], bert_rouge['ROUGE-L']]\n",
    "}\n",
    "\n",
    "# Imprimir tabla de resultados\n",
    "result = pd.DataFrame(data)\n",
    "print(result.to_string(index=False, float_format='{:.4f}'.format))"
   ],
   "id": "4d1ce492d5d434f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Modelo  ROUGE-1  ROUGE-2  ROUGE-L\n",
      "   TF-IDF   0.2817   0.0284   0.1502\n",
      " TextRank   0.3314   0.0809   0.2057\n",
      "Combinado   0.3218   0.1279   0.2184\n",
      "     BERT   0.2420   0.0258   0.1529\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
