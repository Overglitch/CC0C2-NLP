{
 "cells": [
  {
   "cell_type": "code",
   "id": "fb78ea6de943c6e6",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-16T18:32:48.351190Z",
     "start_time": "2024-11-16T18:32:45.204409Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:32:48.366968Z",
     "start_time": "2024-11-16T18:32:48.352199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"\n",
    "    Clase para manejar el vocabulario y las conversiones entre palabras e índices.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializa el vocabulario con el token de padding.\n",
    "        \"\"\"\n",
    "        self.word2idx = {\"<PAD>\": 0}  # Token de padding asignado al índice 0\n",
    "        self.idx2word = {0: \"<PAD>\"}\n",
    "        self.idx = 1  # Iniciar índices de palabras desde 1\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Añade todas las palabras de una oración al vocabulario.\n",
    "\n",
    "        Args:\n",
    "            sentence (list): Lista de palabras de una oración.\n",
    "        \"\"\"\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        \"\"\"\n",
    "        Añade una palabra al vocabulario si no está presente.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a añadir.\n",
    "        \"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retorna el número de palabras en el vocabulario.\n",
    "\n",
    "        Returns:\n",
    "            int: Tamaño del vocabulario.\n",
    "        \"\"\"\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        \"\"\"\n",
    "        Convierte una palabra a su índice correspondiente.\n",
    "\n",
    "        Args:\n",
    "            word (str): Palabra a convertir.\n",
    "\n",
    "        Returns:\n",
    "            int: Índice de la palabra en el vocabulario.\n",
    "        \"\"\"\n",
    "        return self.word2idx.get(word, self.word2idx[\"<PAD>\"])\n",
    "\n",
    "    def index_to_word(self, idx):\n",
    "        \"\"\"\n",
    "        Convierte un índice a su palabra correspondiente.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Índice a convertir.\n",
    "\n",
    "        Returns:\n",
    "            str: Palabra correspondiente al índice.\n",
    "        \"\"\"\n",
    "        return self.idx2word.get(idx, \"<PAD>\")\n",
    "\n",
    "    def sentence_to_indices(self, sentence):\n",
    "        \"\"\"\n",
    "        Convierte una oración en una lista de índices.\n",
    "\n",
    "        Args:\n",
    "            sentence (list): Lista de palabras de una oración.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de índices correspondientes a las palabras.\n",
    "        \"\"\"\n",
    "        return [self.word_to_index(word) for word in sentence]\n",
    "\n",
    "    def indices_to_sentence(self, indices):\n",
    "        \"\"\"\n",
    "        Convierte una lista de índices en una oración.\n",
    "\n",
    "        Args:\n",
    "            indices (list): Lista de índices.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de palabras correspondientes a los índices.\n",
    "        \"\"\"\n",
    "        return [self.index_to_word(idx) for idx in indices]\n"
   ],
   "id": "ceb89d32555f23bf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:32:48.381973Z",
     "start_time": "2024-11-16T18:32:48.368038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para cargar y preprocesar datos de texto.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepath, seq_length=5, num_sentences=None):\n",
    "        \"\"\"\n",
    "        Inicializa el dataset.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Ruta al archivo de texto.\n",
    "            seq_length (int): Longitud de las secuencias de entrada.\n",
    "            num_sentences (int, optional): Número de oraciones a utilizar. Si es None, utiliza todas.\n",
    "        \"\"\"\n",
    "        self.filepath = filepath\n",
    "        self.seq_length = seq_length\n",
    "        self.num_sentences = num_sentences\n",
    "        self.vocab = Vocabulary()  # Instancia de la clase Vocabulary\n",
    "        self.data = self.load_and_preprocess_data()  # Lista de oraciones tokenizadas\n",
    "        self.inputs, self.targets = self.create_sequences()  # Tensores de entradas y etiquetas\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Carga y preprocesa los datos de texto.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de oraciones tokenizadas.\n",
    "        \"\"\"\n",
    "        # Leer el archivo de texto\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Limpiar el texto\n",
    "        text = self.clean_text(text)\n",
    "\n",
    "        # Tokenizar el texto en oraciones\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # Limitar el número de oraciones si es necesario\n",
    "        if self.num_sentences:\n",
    "            sentences = sentences[:self.num_sentences]\n",
    "\n",
    "        # Tokenizar cada oración en palabras\n",
    "        tokenized_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "\n",
    "        # Construir el vocabulario\n",
    "        for sentence in tokenized_sentences:\n",
    "            self.vocab.add_sentence(sentence)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Realiza limpieza del texto eliminando caracteres y patrones no deseados.\n",
    "\n",
    "        Args:\n",
    "            text (str): Texto a limpiar.\n",
    "\n",
    "        Returns:\n",
    "            str: Texto limpio.\n",
    "        \"\"\"\n",
    "        # Eliminar texto dentro de paréntesis\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        # Eliminar signos de igual y tokens desconocidos\n",
    "        text = re.sub(r'=', '', text)\n",
    "        text = re.sub(r'<unk>', '', text)\n",
    "        # Reemplazar múltiples guiones por un espacio\n",
    "        text = re.sub(r'-{2,}', ' ', text)\n",
    "        # Reemplazar múltiples puntos por un solo punto\n",
    "        text = re.sub(r'\\.{2,}', '.', text)\n",
    "        # Eliminar caracteres no deseados, manteniendo letras, números, espacios, puntos y apóstrofes\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\']\", '', text)\n",
    "        # Reemplazar múltiples espacios por uno solo\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def create_sequences(self):\n",
    "        \"\"\"\n",
    "        Crea secuencias de entrada y etiquetas para el entrenamiento.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tensores de entradas y etiquetas.\n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        for sentence in self.data:\n",
    "            # Ignorar oraciones muy cortas\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "\n",
    "            # Convertir la oración en índices\n",
    "            indices = self.vocab.sentence_to_indices(sentence)\n",
    "\n",
    "            # Crear pares de secuencias y objetivos\n",
    "            for i in range(1, len(indices)):\n",
    "                # Obtener la secuencia de entrada de longitud fija\n",
    "                seq = indices[max(0, i - self.seq_length):i]\n",
    "                # Aplicar padding si es necesario\n",
    "                seq = [0] * (self.seq_length - len(seq)) + seq  # Padding con ceros (índice de \"<PAD>\")\n",
    "                inputs.append(seq)\n",
    "                # El objetivo es la palabra actual\n",
    "                targets.append(indices[i])\n",
    "\n",
    "        # Convertir las listas en tensores\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        return inputs, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Retorna el número total de muestras en el dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Número de muestras.\n",
    "        \"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retorna la muestra en la posición dada.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Índice de la muestra.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Secuencia de entrada y etiqueta correspondiente.\n",
    "        \"\"\"\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ],
   "id": "f0ada196bc3f3ffd",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:32:48.396973Z",
     "start_time": "2024-11-16T18:32:48.382975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GaussianEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementación de embeddings gaussianos para representar palabras como distribuciones.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Inicializa los embeddings gaussianos.\n",
    "\n",
    "        Args:\n",
    "            num_embeddings (int): Número de embeddings (tamaño del vocabulario).\n",
    "            embedding_dim (int): Dimensión de los embeddings.\n",
    "            padding_idx (int, optional): Índice para el token de padding.\n",
    "        \"\"\"\n",
    "        super(GaussianEmbedding, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # Embeddings para la media y la log-varianza de cada palabra\n",
    "        self.mean = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "        self.log_var = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # Inicializar los pesos de los embeddings de forma uniforme\n",
    "        nn.init.uniform_(self.mean.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.log_var.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante obteniendo los embeddings muestreados.\n",
    "\n",
    "        Args:\n",
    "            input (Tensor): Tensor de índices de palabras.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tensor muestreado z, medias y log-varianzas.\n",
    "        \"\"\"\n",
    "        # Obtener la media y la log-varianza para los índices de entrada\n",
    "        mean = self.mean(input)\n",
    "        log_var = self.log_var(input)\n",
    "\n",
    "        # Calcular la desviación estándar\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "\n",
    "        # Muestrear epsilon de una distribución normal estándar\n",
    "        epsilon = torch.randn_like(std)\n",
    "\n",
    "        # Aplicar el truco de reparametrización\n",
    "        z = mean + epsilon * std\n",
    "\n",
    "        return z, mean, log_var\n",
    "\n",
    "    def kl_loss(self, mean, log_var):\n",
    "        \"\"\"\n",
    "        Calcula la pérdida de divergencia KL entre el embedding gaussiano y una normal estándar.\n",
    "\n",
    "        Args:\n",
    "            mean (Tensor): Medias de los embeddings.\n",
    "            log_var (Tensor): Log-varianzas de los embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Pérdida KL promedio.\n",
    "        \"\"\"\n",
    "        # Calcular la divergencia KL para cada palabra en la secuencia\n",
    "        kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), dim=2)\n",
    "        # Retornar la pérdida KL promedio\n",
    "        return kl.mean()\n"
   ],
   "id": "efd84852e320832b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:32:48.412974Z",
     "start_time": "2024-11-16T18:32:48.397974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo LSTM que utiliza embeddings gaussianos para modelar secuencias de texto.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx, dropout_p):\n",
    "        \"\"\"\n",
    "        Inicializa el modelo LSTM.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Tamaño del vocabulario.\n",
    "            embedding_dim (int): Dimensión de los embeddings.\n",
    "            hidden_dim (int): Dimensión de las capas ocultas del LSTM.\n",
    "            padding_idx (int): Índice del token de padding.\n",
    "            dropout_p (float): Probabilidad de dropout.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        # Embeddings gaussianos\n",
    "        self.embedding = GaussianEmbedding(num_embeddings=vocab_size, embedding_dim=embedding_dim,\n",
    "                                           padding_idx=padding_idx)\n",
    "        # LSTM unidireccional de una capa\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        # Dropout para regularización\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        # Capa totalmente conectada para mapear a los logits de salida\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Realiza el paso hacia adelante del modelo.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Tensor de secuencias de entrada (índices de palabras).\n",
    "            hidden (tuple): Estados ocultos iniciales del LSTM.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Salidas del modelo, estados ocultos finales, medias y log-varianzas de los embeddings.\n",
    "        \"\"\"\n",
    "        # Obtener los embeddings gaussianos muestreados\n",
    "        x, mean, log_var = self.embedding(x)\n",
    "        # Pasar los embeddings a través del LSTM\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        # Aplicar dropout\n",
    "        x = self.dropout(x)\n",
    "        # Tomar la salida del último paso temporal\n",
    "        x = x[:, -1, :]\n",
    "        # Calcular los logits de salida\n",
    "        x = self.fc(x)\n",
    "        return x, hidden, mean, log_var\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"\n",
    "        Inicializa los estados ocultos del LSTM con ceros.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Tamaño del lote.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Estados ocultos iniciales (h_0, c_0).\n",
    "        \"\"\"\n",
    "        # Obtener el tipo de dato del modelo\n",
    "        weight = next(self.parameters()).data\n",
    "        # Inicializar estados ocultos h_0 y c_0\n",
    "        return (weight.new_zeros(1, batch_size, self.lstm.hidden_size),\n",
    "                weight.new_zeros(1, batch_size, self.lstm.hidden_size))\n"
   ],
   "id": "290895085fcf3022",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:32:48.428974Z",
     "start_time": "2024-11-16T18:32:48.413975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Clase para entrenar el modelo y generar texto.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, train_dataset, batch_size=64, lr=0.001, clip=5, kl_weight=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa el entrenador.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): Modelo a entrenar.\n",
    "            train_dataset (Dataset): Conjunto de datos de entrenamiento.\n",
    "            batch_size (int, optional): Tamaño del lote.\n",
    "            lr (float, optional): Tasa de aprendizaje.\n",
    "            clip (float, optional): Valor para clipping de gradientes.\n",
    "            kl_weight (float, optional): Peso para la pérdida de divergencia KL.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        # Cargador de datos para el entrenamiento\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.vocab = train_dataset.vocab\n",
    "        self.seq_length = train_dataset.seq_length\n",
    "        # Función de pérdida (ignora el índice del token de padding)\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        # Optimizador Adam\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.clip = clip  # Valor para clipping de gradientes\n",
    "        self.kl_weight = kl_weight  # Peso para la pérdida de divergencia KL\n",
    "\n",
    "    def train(self, epochs):\n",
    "        \"\"\"\n",
    "        Entrena el modelo durante un número de épocas.\n",
    "\n",
    "        Args:\n",
    "            epochs (int): Número de épocas de entrenamiento.\n",
    "        \"\"\"\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()  # Establecer el modo de entrenamiento\n",
    "            epoch_loss = 0  # Acumulador de pérdida para la época\n",
    "\n",
    "            for inputs, targets in self.train_loader:\n",
    "                batch_size = inputs.size(0)\n",
    "                # Inicializar los estados ocultos del LSTM\n",
    "                hidden = self.model.init_hidden(batch_size)\n",
    "                # Desconectar los estados ocultos para evitar problemas de gradientes\n",
    "                hidden = tuple([h.data for h in hidden])\n",
    "\n",
    "                # Reiniciar los gradientes del modelo\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Pasar las entradas a través del modelo\n",
    "                outputs, hidden, mean, log_var = self.model(inputs, hidden)\n",
    "\n",
    "                # Calcular la pérdida de entropía cruzada (NLLLoss)\n",
    "                nll_loss = self.criterion(outputs, targets)\n",
    "\n",
    "                # Calcular la pérdida de divergencia KL\n",
    "                kl_loss = self.model.embedding.kl_loss(mean, log_var)\n",
    "\n",
    "                # Pérdida total\n",
    "                loss = nll_loss + self.kl_weight * kl_loss\n",
    "\n",
    "                # Backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Aplicar clipping de gradientes para evitar explosión de gradientes\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.clip)\n",
    "\n",
    "                # Actualizar los parámetros del modelo\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Acumular la pérdida de la época\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "\n",
    "            # Calcular la pérdida promedio y la perplejidad\n",
    "            avg_loss = epoch_loss / len(self.train_loader.dataset)\n",
    "            perplexity = np.exp(avg_loss)\n",
    "\n",
    "            print(f'Epoch {epoch}, Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}')\n",
    "\n",
    "            # Generar texto después de cada época\n",
    "            generated_text = self.generate_text('The', 50, top_k=5)\n",
    "            print(f'Text generated after epoch {epoch}:\\n{generated_text}\\n')\n",
    "\n",
    "    def generate_text(self, init_text, length, top_k=5):\n",
    "        \"\"\"\n",
    "        Genera texto utilizando el modelo entrenado.\n",
    "\n",
    "        Args:\n",
    "            init_text (str): Texto inicial para comenzar la generación.\n",
    "            length (int): Número de palabras a generar.\n",
    "            top_k (int, optional): Número de opciones más probables para muestrear la siguiente palabra.\n",
    "\n",
    "        Returns:\n",
    "            str: Texto generado.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Establecer el modo de evaluación\n",
    "        words = word_tokenize(init_text)  # Tokenizar el texto inicial\n",
    "        state_h, state_c = self.model.init_hidden(1)  # Inicializar estados ocultos\n",
    "\n",
    "        for _ in range(length):\n",
    "            # Obtener las últimas (seq_length - 1) palabras como entrada\n",
    "            input_words = words[-(self.seq_length - 1):]\n",
    "            # Convertir palabras a índices\n",
    "            indices = [self.vocab.word_to_index(w) for w in input_words]\n",
    "            # Aplicar padding si es necesario\n",
    "            if len(indices) < self.seq_length - 1:\n",
    "                indices = [0] * (self.seq_length - 1 - len(indices)) + indices\n",
    "\n",
    "            # Convertir a tensor\n",
    "            x = torch.tensor([indices], dtype=torch.long)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Pasar la entrada a través del modelo\n",
    "                output, (state_h, state_c), mean, log_var = self.model(x, (state_h, state_c))\n",
    "                # Calcular las probabilidades\n",
    "                probs = F.softmax(output, dim=1).data\n",
    "\n",
    "                # Obtener las top_k palabras más probables\n",
    "                top_probs, top_ix = probs.topk(top_k)\n",
    "\n",
    "                # Convertir a numpy y aplanar\n",
    "                top_probs = top_probs.cpu().numpy().squeeze()\n",
    "                top_ix = top_ix.cpu().numpy().squeeze()\n",
    "\n",
    "                # Muestrear la siguiente palabra de las top_k opciones\n",
    "                word_idx = np.random.choice(top_ix, p=top_probs / top_probs.sum())\n",
    "                # Convertir índice a palabra\n",
    "                word = self.vocab.index_to_word(word_idx)\n",
    "                # Añadir la palabra generada a la lista\n",
    "                words.append(word)\n",
    "\n",
    "        # Unir las palabras en una cadena de texto\n",
    "        return ' '.join(words)\n"
   ],
   "id": "d07cfba422bde43a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:53:55.903628Z",
     "start_time": "2024-11-16T18:32:48.429975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definir parámetros del modelo y entrenamiento\n",
    "seq_length = 6  # Longitud de las secuencias de entrada\n",
    "num_sentences = 20000  # Número de oraciones a utilizar del conjunto de datos\n",
    "train_filepath = 'resources/data/wikitext2/train.txt'  # Ruta al archivo de texto\n",
    "#train_filepath = 'resources/data/wikitext2/baby.txt'  # Ruta al archivo de texto\n",
    "# Crear el dataset de entrenamiento\n",
    "train_dataset = TextDataset(train_filepath, seq_length=seq_length, num_sentences=num_sentences)\n",
    "\n",
    "print(f\"Número de secuencias de entrada: {len(train_dataset)}\")\n",
    "\n",
    "# Parámetros del modelo\n",
    "vocab_size = len(train_dataset.vocab)  # Tamaño del vocabulario\n",
    "embedding_dim = 100  # Dimensión de los embeddings\n",
    "hidden_dim = 128  # Dimensión de las capas ocultas del LSTM\n",
    "padding_idx = train_dataset.vocab.word_to_index(\"<PAD>\")  # Índice del token de padding\n",
    "dropout_p = 0.1  # Probabilidad de dropout\n",
    "batch_size = 200  # Tamaño del lote\n",
    "learning_rate = 0.001  # Tasa de aprendizaje\n",
    "num_epochs = 10  # Número de épocas de entrenamiento\n",
    "\n",
    "# Instanciar el modelo con embeddings gaussianos\n",
    "model = LSTMModel(vocab_size=vocab_size, embedding_dim=embedding_dim,\n",
    "                  hidden_dim=hidden_dim, padding_idx=padding_idx, dropout_p=dropout_p)\n",
    "\n",
    "# Instanciar el entrenador\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset, batch_size=batch_size,\n",
    "                  lr=learning_rate, kl_weight=0.1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train(num_epochs)\n"
   ],
   "id": "62680df548ba307f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de secuencias de entrada: 435065\n",
      "Epoch 1, Loss: 7.4478, Perplexity: 1716.0271\n",
      "Text generated after epoch 1:\n",
      "The in of . the and the in the the the . . the the and of . of . in the . the . . . . . the the of the . of of the the of in . the . . and . of a the the .\n",
      "\n",
      "Epoch 2, Loss: 7.2056, Perplexity: 1346.9440\n",
      "Text generated after epoch 2:\n",
      "The States and the song the and . the . of the . of in . the the and century the city to the first of the song to . . . in the city of the and of the and . and a the of . of the city of\n",
      "\n",
      "Epoch 3, Loss: 7.1027, Perplexity: 1215.2363\n",
      "Text generated after epoch 3:\n",
      "The first and . of the United first . in the the . . of in the . . . to the game of the and and in the city of the . of the time of the United century of the and in a . . in the first .\n",
      "\n",
      "Epoch 4, Loss: 7.0408, Perplexity: 1142.2913\n",
      "Text generated after epoch 4:\n",
      "The episode of the first and . of the United States and in the and in and and . of the a States of the a and and the . of a and of the first and the first the and . was in the first of the . . of\n",
      "\n",
      "Epoch 5, Loss: 6.9869, Perplexity: 1082.3467\n",
      "Text generated after epoch 5:\n",
      "The and was is a the film and the . of his . of the and . . . the of the game . to a city of a and 's . of the city . in the first and and the Cup of the . . in the end of\n",
      "\n",
      "Epoch 6, Loss: 6.9337, Perplexity: 1026.2890\n",
      "Text generated after epoch 6:\n",
      "The of of the . and in the and . . of his and . and the and of of a and in the United States . was a and the . of the same . of a and . . was a in a . of the and and and\n",
      "\n",
      "Epoch 7, Loss: 6.8802, Perplexity: 972.8026\n",
      "Text generated after epoch 7:\n",
      "The episode are the city of the first . of the first and the first of a United of his Places of the United States . was been been the first the . of the first of the and of the season the . . and to the first of the\n",
      "\n",
      "Epoch 8, Loss: 6.8331, Perplexity: 928.0627\n",
      "Text generated after epoch 8:\n",
      "The new day of the United States . of the United century in the time of which . and his . of the . . century . in the of . in a city and the and in the first . and a . of the own and War the .\n",
      "\n",
      "Epoch 9, Loss: 6.7905, Perplexity: 889.3697\n",
      "Text generated after epoch 9:\n",
      "The first of the city . was not a of the the River . . . was a . . was to the and of the first year . is a only . of his first and the first in the . of a first and in . . and not\n",
      "\n",
      "Epoch 10, Loss: 6.7457, Perplexity: 850.4138\n",
      "Text generated after epoch 10:\n",
      "The episode of the . was the the of and . . is not the first and to been a first and of the second season of the and . of the United States and the and . . is a in the season and the and . of a and\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:53:56.619886Z",
     "start_time": "2024-11-16T18:53:55.904646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generar texto después del entrenamiento\n",
    "init_text = 'Once upon a time'  # Texto inicial para la generación\n",
    "generated_text = trainer.generate_text(init_text, length=1000, top_k=5)\n",
    "print(\"Texto generado después del entrenamiento:\")\n",
    "print(generated_text)\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado después del entrenamiento:\n",
      "Once upon a time of the city . was a only to the city . is the largest and and the new of the . 's Olympics and the city of the and and and a first . . had the most time of the . of a United of a first in . and the song . . was the only of . 's . 's . is not a in . of a . and and of a time to the first of . 's World Kombat . and and and . was not not in the and . 's World century . and his . . had a in the . and the song and a new of the . of the and . . in the first . of the series of the series of a and the album . in the time of the . of the Missouri of . 's . of the new River of the and . of a second time and the of the and of the United century of a United States . . in the first and the and of the Missouri River was the and of the United Kingdom of October a city 's first the and of his second . was a large to the of the the season and the and . State in and a in a time the second year . and to the first American . was a first and . . is not been the only of the Song of the United States and the second season of the Missouri . . is a great in . . was a first and the of the second . was been found . . of her first than the first . is a series the ball 's and in the United States in the city of the first season and in a in . . was not to be a most and . and a first of the first and and in the first in and to a first . and the series the song was a and to the first . and the first . and a song was a first . of . . 's and in the time . was a new in the game . of the first and of the United States in the world . . in the city of the United of . and a . 's second and a of the city was the first 's of . . and a first . of the . is the in the first . of the Missouri River the and . of two . and a the character of . and a new in the Missouri States and the to the first . . is a large and and and the first of the series . and a in the first . . and the series of a first River . . and a character . was the only of a city and the of to a first and . was not a in . . of a and . is be . was be the in of the United century the and of the . 's and the and and the song . . and . . . was no very . . is been a great to the city 's the first of . and the first and . . is be a most to . . of a first . 's . . had to the in the of . . of the city . had a second and of a season was a first and . . of the new . and a first . of the United States . of a city and the first of the second the . . is the second of . of the second time of the United of the city of the city . 's second and and a first of a and year of a United of the city the the of a . . is the in and and the game of the second River . of the first of . and a and . was a by the . River of the United States and the of in . States the song 's first the in the United States and the song and in the of the second . and the first River was a to the first and . was a in the city of the of of the Missouri and and a first of and and . is the only . of the series and the series of the . . was not the only in a second . . was the in of and . . was the and and . 's the character and its first of the first . was been a most and . of a first season . was not been the to to a own and and a city of an city and and the . is the first character and the series of which . is a only in of the first of . and . . is a first and and a first season . of in 1 and the . 's a of and of the and and in the city . was a in the and . and his of the Missouri . of the the season was was been and to the first of . and a first than the . of the first of a Missouri . and the Fantasy 38 and the city and the season and and . and the song of his city . was the . . was the only to the . of the first . as the of . of the series 's album was the first in . century the game in the second . . and to the first . and the first of . 's World Church . in the and of the of .\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:56:04.423265Z",
     "start_time": "2024-11-16T18:56:03.551020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generar texto después del entrenamiento\n",
    "init_text = 'The'  # Texto inicial para la generación\n",
    "generated_text = trainer.generate_text(init_text, length=1000, top_k=5)\n",
    "print(\"Texto generado después del entrenamiento:\")\n",
    "print(generated_text)"
   ],
   "id": "4b1b0f73bc72e67a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado después del entrenamiento:\n",
      "The first is a and in the second and the and . was the a . of its United States and a world and the . and the song of a . . and a . 's of the first of the and . of a second . was a by . . . was the first in in a . . of a second of and . is and in the city and a and of the series of a and of in 000 . was also by . 's and and . was the of of a and . . of its and . . . was been been first . was been be been in the and in a second season . had been to be to the and of . was not a first . of the city of his and of the Kombat and and the in of . and a first . is the . States in the . . is in an . of the and . was the a and of to the United States . and the first to a first of the game of the city 's and . is not the of to a second and . . had a only of the first of the of the United States and the and . and in a game . was the only of the first of a American and . was had a first of the first and . and a second in the United States . of the Missouri of a . of the United century in the first of . is the first of the of . and the the of . of the time in a series in the . of the city 's of the city and a series in the second of . . was a first of of the game of the first of the first and in October . . of the . of the first River and the and . . is the first of a second . of the first in the United States and a United League . . in the city . is to a new and . States was and a and in the first and the series and . . had a only and the and of the Fantasy of the the World League Championship Championship and a . in the game . and a time . . is to be to in the United States . was the series for the large and and . in a game . had a first time of the first the victory . of the United century . was to a and . and was a first and in the city . States . was not not the in the United century . . and the in the . . was the result of a first . in the first . . was a than . States . and the first . . . was a most . of the first of the Missouri in the and of . 's first . was not the largest . and the . is the first . of the first and the Missouri of the first 0 . and a first . of a city . was a largest woman of his . . of the city of to the of and . 's . and the first of the of the city of the . . was the first and of the and of the United States the and of the and . States . was been found for the end the and and a song in in their . of the first . . was the first and of in the song was . . in the game and a first year of a second . . was that to the second . of a series of the own in a United of the World VII . was a first in the second . and the first and the song of the Fantasy Cup of the United States and the in the . and the in his first . . was a only the series . was the large a and and . was the . of the first in which as the city and the of . of a series and the and the second . River . of a United States in the city of the United States and the song was a second of . in the first time . of that a second and and . of a United Kingdom in the . 's World Cup and a and in . of the . of the song and he . had not . of the . . was not a . of the game . had not a in a first and a second season in the season . was been been not . 's the and . 's first and the in a time . was a . States was not the . 's song a was in the United States . . and . was the and and the only a first of the second . of the series of the season . was the in and the city was the to a of a . and the first . . . is the first . of the war of the United States . and the . 's . and the city and his of the first . of the first . and the of the Missouri . . of his city . . and been the . is not the only the and and in a time . is not a first of a the and . of that . . of the time . of the song and the own River in the United States the Historic II and a and . . 's the first . was a a . . was\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T18:58:19.314537Z",
     "start_time": "2024-11-16T18:58:19.281050Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(), 'model/lstm_gauss_embedding.pth')",
   "id": "64e0b1300821fda1",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
