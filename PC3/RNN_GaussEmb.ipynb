{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings Gaussianos y RNNs para Modelado de Incertidumbre en Secuencias de Texto",
   "id": "b50e12fe71a105b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Implementación en PyTorch de una RNN que utiliza gaussian embeddings para generar texto de manera probabilística, permitiendo la captura de múltiples posibles continuaciones de una secuencia.\n",
    "\n",
    "Combina gaussian embeddings con RNNs para capturar la incertidumbre y variabilidad en la generación de secuencias de texto, mejorando la robustez del modelo."
   ],
   "id": "4661633246b5701b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:24:59.326767Z",
     "start_time": "2024-11-16T17:24:57.143401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ],
   "id": "ba47d7e3a440d380",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:25:03.680935Z",
     "start_time": "2024-11-16T17:25:03.672923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Vocabulary: \n",
    "    def __init__(self):\n",
    "        self.word2idx = {\"<PAD>\": 0}\n",
    "        self.idx2word = {0: \"<PAD>\"}\n",
    "        self.idx = 1 \n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def word_to_index(self, word):\n",
    "        return self.word2idx.get(word, self.word2idx[\"<PAD>\"])\n",
    "\n",
    "    def index_to_word(self, idx):\n",
    "        return self.idx2word.get(idx, \"<PAD>\")\n",
    "\n",
    "    def sentence_to_indices(self, sentence):\n",
    "        return [self.word_to_index(word) for word in sentence]\n",
    "\n",
    "    def indices_to_sentence(self, indices):\n",
    "        return [self.index_to_word(idx) for idx in indices]\n"
   ],
   "id": "3a75a2902a54b0ae",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:25:08.896194Z",
     "start_time": "2024-11-16T17:25:08.887686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, filepath, seq_length=5, num_sentences=None):\n",
    "        self.filepath = filepath\n",
    "        self.seq_length = seq_length\n",
    "        self.num_sentences = num_sentences\n",
    "        self.vocab = Vocabulary()\n",
    "        self.data = self.load_and_preprocess_data()\n",
    "        self.inputs, self.targets = self.create_sequences()\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        with open(self.filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "\n",
    "        text = self.clean_text(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        if self.num_sentences:\n",
    "            sentences = sentences[:self.num_sentences]\n",
    "\n",
    "        tokenized_sentences = [word_tokenize(sent) for sent in sentences]\n",
    "        for sentence in tokenized_sentences:\n",
    "            self.vocab.add_sentence(sentence)\n",
    "\n",
    "        return tokenized_sentences\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Eliminar texto dentro de paréntesis\n",
    "        text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "        # Eliminar signos de igual y tokens desconocidos\n",
    "        text = re.sub(r'=', '', text)\n",
    "        text = re.sub(r'<unk>', '', text)\n",
    "        # Reemplazar múltiples guiones por un espacio\n",
    "        text = re.sub(r'-{2,}', ' ', text)\n",
    "        # Reemplazar múltiples puntos por un solo punto\n",
    "        text = re.sub(r'\\.{2,}', '.', text)\n",
    "        # Eliminar caracteres no deseados\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\']\", '', text)\n",
    "        # Reemplazar múltiples espacios por uno solo\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "\n",
    "    def create_sequences(self):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for sentence in self.data:\n",
    "            if len(sentence) < 2:\n",
    "                continue\n",
    "            indices = self.vocab.sentence_to_indices(sentence)\n",
    "            for i in range(1, len(indices)):\n",
    "                seq = indices[max(0, i - self.seq_length):i]\n",
    "                seq = [0] * (self.seq_length - len(seq)) + seq  # Padding\n",
    "                inputs.append(seq)\n",
    "                targets.append(indices[i])\n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.long)\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "        return inputs, targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n"
   ],
   "id": "e82ae67c611811a3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:25:15.144912Z",
     "start_time": "2024-11-16T17:25:15.137826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GaussianEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=0):\n",
    "        super(GaussianEmbedding, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # Parámetros de media y log-varianza para cada palabra\n",
    "        self.mean = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "        self.log_var = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # Inicializar embeddings\n",
    "        nn.init.uniform_(self.mean.weight, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.log_var.weight, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        mean = self.mean(input)\n",
    "        log_var = self.log_var(input)\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "\n",
    "        # Muestrear epsilon de una distribución normal estándar\n",
    "        epsilon = torch.randn_like(std)\n",
    "        # Truco de reparametrización\n",
    "        z = mean + epsilon * std\n",
    "\n",
    "        return z, mean, log_var\n",
    "\n",
    "    def kl_loss(self, mean, log_var):\n",
    "        # Calcular la divergencia KL entre el embedding gaussiano y una normal estándar\n",
    "        kl = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp(), dim=2)\n",
    "        return kl.mean()\n"
   ],
   "id": "3e84e416af77d4c9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:25:21.685824Z",
     "start_time": "2024-11-16T17:25:21.667467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx, dropout_p):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = GaussianEmbedding(num_embeddings=vocab_size, embedding_dim=embedding_dim,\n",
    "                                           padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x, mean, log_var = self.embedding(x)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x[:, -1, :])  # Usar la salida del último paso temporal\n",
    "        return x, hidden, mean, log_var\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new_zeros(1, batch_size, self.lstm.hidden_size),\n",
    "                weight.new_zeros(1, batch_size, self.lstm.hidden_size))\n"
   ],
   "id": "771e84ca490b8d53",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:25:26.240167Z",
     "start_time": "2024-11-16T17:25:26.183627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_dataset, batch_size=64, lr=0.001, clip=5, kl_weight=0.1):\n",
    "        self.model = model\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.vocab = train_dataset.vocab\n",
    "        self.seq_length = train_dataset.seq_length\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=0) # Ignorar el padding\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        self.clip = clip\n",
    "        self.kl_weight = kl_weight  # Peso para la pérdida de divergencia Kellback-Leibler\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for inputs, targets in self.train_loader:\n",
    "                batch_size = inputs.size(0)\n",
    "                hidden = self.model.init_hidden(batch_size)\n",
    "                hidden = tuple([h.data for h in hidden])\n",
    "\n",
    "                self.model.zero_grad()\n",
    "                outputs, hidden, mean, log_var = self.model(inputs, hidden)\n",
    "                nll_loss = self.criterion(outputs, targets) # Pérdida de entropía cruzada\n",
    "                kl_loss = self.model.embedding.kl_loss(mean, log_var) # Pérdida de divergencia KL\n",
    "                loss = nll_loss + self.kl_weight * kl_loss # Pérdida total\n",
    "                loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self.clip) # Clip de gradientes\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item() * batch_size\n",
    "\n",
    "            avg_loss = epoch_loss / len(self.train_loader.dataset)\n",
    "            perplexity = np.exp(avg_loss) # incertidumbre del modelo en la predicción de palabras\n",
    "            print(f'Epoch {epoch}, Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}')\n",
    "\n",
    "            generated_text = self.generate_text('The', 50, top_k=5)\n",
    "            print(f'Text generated after epoch {epoch}:\\n{generated_text}\\n')\n",
    "\n",
    "    def generate_text(self, init_text, length, top_k=5):\n",
    "        self.model.eval()\n",
    "        words = word_tokenize(init_text)\n",
    "        state_h, state_c = self.model.init_hidden(1)\n",
    "        for _ in range(length):\n",
    "            input_words = words[-(self.seq_length - 1):]\n",
    "            indices = [self.vocab.word_to_index(w) for w in input_words]\n",
    "            if len(indices) < self.seq_length - 1:\n",
    "                indices = [0] * (self.seq_length - 1 - len(indices)) + indices\n",
    "\n",
    "            x = torch.tensor([indices], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                output, (state_h, state_c), mean, log_var = self.model(x, (state_h, state_c))\n",
    "                probs = F.softmax(output, dim=1).data\n",
    "                # Muestreo de la distribución\n",
    "                top_probs, top_ix = probs.topk(top_k)\n",
    "                top_probs = top_probs.cpu().numpy().squeeze()\n",
    "                top_ix = top_ix.cpu().numpy().squeeze()\n",
    "                word_idx = np.random.choice(top_ix, p=top_probs / top_probs.sum())\n",
    "                word = self.vocab.index_to_word(word_idx)\n",
    "                words.append(word)\n",
    "\n",
    "        return ' '.join(words)\n"
   ],
   "id": "d619144da395d46",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:28:23.006509Z",
     "start_time": "2024-11-16T17:26:28.810782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Definir parámetros\n",
    "seq_length = 6\n",
    "num_sentences = 25000 \n",
    "train_filepath = 'resources/data/wikitext2/baby.txt'\n",
    "\n",
    "# Crear dataset\n",
    "train_dataset = TextDataset(train_filepath, seq_length=seq_length, num_sentences=num_sentences)\n",
    "\n",
    "print(f\"Número de secuencias de entrada: {len(train_dataset)}\")\n",
    "\n",
    "# Actualizar vocab_size y padding_idx\n",
    "vocab_size = len(train_dataset.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "padding_idx = train_dataset.vocab.word_to_index(\"<PAD>\")\n",
    "dropout_p = 0.1\n",
    "batch_size = 200\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Instanciar el modelo con Gaussian Embeddings\n",
    "model = LSTMModel(vocab_size=vocab_size, embedding_dim=embedding_dim,\n",
    "                  hidden_dim=hidden_dim, padding_idx=padding_idx, dropout_p=dropout_p)\n",
    "\n",
    "# Instanciar el trainer con el nuevo modelo\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset, batch_size=batch_size, lr=learning_rate, kl_weight=0.1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train(num_epochs)\n"
   ],
   "id": "3d7a90c5472e3f9a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de secuencias de entrada: 107740\n",
      "Epoch 1, Loss: 7.4279, Perplexity: 1682.2655\n",
      "Text generated after epoch 1:\n",
      "The . the the the and the . of . to . the to . the of . the of . and the and the the . of the and the . and the of . of . . . of to the . the . of of . the and\n",
      "\n",
      "Epoch 2, Loss: 7.1032, Perplexity: 1215.8862\n",
      "Text generated after epoch 2:\n",
      "The in . . to of the and and the the of of the the the the . of the . the in the . . to of the the . the . to the in the the the the . in the of the and of the . to in\n",
      "\n",
      "Epoch 3, Loss: 7.0763, Perplexity: 1183.5990\n",
      "Text generated after epoch 3:\n",
      "The of the the the of . of of of the . . . the the in . and to of to and and the the . a to and . the the to the of of the and . to of the the and . the of the . and\n",
      "\n",
      "Epoch 4, Loss: 7.0363, Perplexity: 1137.1417\n",
      "Text generated after epoch 4:\n",
      "The a . of and the the the of and a . . . the . in . of and . a the and . the the the the a . . the the a . of the in in the a of . a the the and the and a\n",
      "\n",
      "Epoch 5, Loss: 6.9716, Perplexity: 1065.9191\n",
      "Text generated after epoch 5:\n",
      "The . of the in . the first the the and and . the and . of the first the and . . . Missouri Missouri the the and the and . the Missouri of and the . of the the . of in to the Missouri of the . and\n",
      "\n",
      "Epoch 6, Loss: 6.9101, Perplexity: 1002.3677\n",
      "Text generated after epoch 6:\n",
      "The the in a the and . in the film . to . . a a and of the and of the and and the Missouri and a and of and the first of and and and of . the film . the Missouri and of the first . . the\n",
      "\n",
      "Epoch 7, Loss: 6.8540, Perplexity: 947.6664\n",
      "Text generated after epoch 7:\n",
      "The city to a film to a river and the first . and of the the film in the Missouri of the first and and . . and a Missouri and a the Missouri the of and the the Fleet of the river . the first of the first . the\n",
      "\n",
      "Epoch 8, Loss: 6.8050, Perplexity: 902.3483\n",
      "Text generated after epoch 8:\n",
      "The the Missouri of a Missouri of the Missouri and of a Missouri of the Missouri of the United States of a Missouri of and . and a film . the film . the first States and . . the the Missouri . . . a Missouri and the a and\n",
      "\n",
      "Epoch 9, Loss: 6.7608, Perplexity: 863.3277\n",
      "Text generated after epoch 9:\n",
      "The ship . the film . a first century a episode in the Missouri . the the season . the film to the United . and the first River in the city and the Missouri River in in the and of the first . . the episode . the Missouri .\n",
      "\n",
      "Epoch 10, Loss: 6.7171, Perplexity: 826.4185\n",
      "Text generated after epoch 10:\n",
      "The first and in the Missouri River the first Missouri . the Missouri . . in the first and the Missouri of a Ganges and and . in the in a Missouri and . the Missouri of a . of his Missouri . the . and . the and and .\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T17:29:24.279554Z",
     "start_time": "2024-11-16T17:29:24.188786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Generar texto después del entrenamiento\n",
    "init_text = 'The'\n",
    "generated_text = trainer.generate_text(init_text, length=100, top_k=5)\n",
    "print(\"Texto generado después del entrenamiento:\")\n",
    "print(generated_text)\n"
   ],
   "id": "5276f99f714ea99a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto generado después del entrenamiento:\n",
      "The first of the of . the episode . . . of the shark . . the Missouri of the Missouri in the and . the Missouri States . the the States . in a Missouri of the . of a river in the river and the . in the Missouri . of the Missouri River of the Missouri 's in . in a in . in the Ganges . and the Missouri of the Missouri and the Missouri . and of the . . the Missouri . . . . be a . . the and was the and was\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
