{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Proyecto 1: Integración de modelos Naive Bayes y regresión logística multinomial para clasificación multiclase con evaluación\n",
    "\n",
    "**Integrantes**:\n",
    "\n",
    "| Apellidos y nombres | Código |\n",
    "|---------------------|--------|\n",
    "| Meza Rodriguez, Fiorella Ivonne | 20192730G |\n",
    "| Murillo Dominguez, Paul Hans | 20193507J |\n",
    "\n",
    "\n",
    "\n",
    "**Descripción**: Desarrolla un sistema de clasificación de texto que combine modelos generativos (Naive Bayes) y discriminativos (Regresión Logística Multinomial) para tareas de clasificación multiclase, como la categorización de noticias. Implementa técnicas de descenso de gradiente estocástico con mini-lotes y regularización para optimizar los modelos. Evalúa el rendimiento utilizando métricas como precisión, recall, medida F y realiza pruebas de significancia estadística, incluyendo la prueba bootstrap pareada.\n",
    "\n",
    "**Resultados esperados**:\n",
    "- Implementación funcional de Naive Bayes y Regresión Logística Multinomial.\n",
    "- Sistema de clasificación que integra ambos modelos para mejorar la precisión.\n",
    "- Optimización mediante descenso de gradiente estocástico con mini-lotes y regularización.\n",
    "- Evaluación exhaustiva utilizando precisión, recall, medida F, y pruebas de significancia.\n",
    "- Análisis comparativo entre modelos generativos y discriminativos.\n",
    "- Documentación detallada de la implementación y los resultados.\n",
    "\n",
    "**Entradas**:\n",
    "- Conjunto de datos etiquetado para clasificación multiclase (e.g., Reuters News Dataset).\n",
    "- Parámetros de optimización y regularización.\n",
    "- Configuraciones para pruebas estadísticas.\n",
    "\n",
    "**Salidas**:\n",
    "- Modelos entrenados de Naive Bayes y Regresión Logística Multinomial.\n",
    "- Reporte de métricas de evaluación (precisión, recall, F1) para cada modelo.\n",
    "- Resultados de pruebas de significancia estadística.\n",
    "- Visualizaciones de la convergencia del descenso de gradiente.\n",
    "- Análisis interpretativo de los modelos."
   ],
   "id": "8ed6de54af339454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:49.587156Z",
     "start_time": "2024-10-17T11:09:48.494153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, hamming_loss\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "d3f9531d0fc3118",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Conjunto de datos etiquetado para clasificación multiclase",
   "id": "8b626ff30f394344"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1. Descarga del conjunto de datos Reuters News Dataset",
   "id": "a00c073e5e325d12"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:49.805407Z",
     "start_time": "2024-10-17T11:09:49.588402Z"
    }
   },
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt_tab')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Overglitch\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2. Carga y etiquetado de los documentos",
   "id": "6f31dd0fb1f150bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:51.108060Z",
     "start_time": "2024-10-17T11:09:49.805407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_documents_and_categories(fileids):\n",
    "    documents = [reuters.raw(file) for file in fileids]\n",
    "    categories = [reuters.categories(file) for file in fileids]\n",
    "    return documents, categories\n",
    "\n",
    "\n",
    "train_fileids = [file for file in reuters.fileids() if file.startswith('training/')]\n",
    "test_fileids = [file for file in reuters.fileids() if file.startswith('test/')]\n",
    "\n",
    "train_documents, train_categories = get_documents_and_categories(train_fileids)\n",
    "test_documents, test_categories = get_documents_and_categories(test_fileids)"
   ],
   "id": "dd5f62d53422af2f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3. Codificación de las etiquetas",
   "id": "aaffa0d538201374"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:51.124111Z",
     "start_time": "2024-10-17T11:09:51.109061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "train_labels = mlb.fit_transform(train_categories)\n",
    "test_labels = mlb.transform(test_categories)"
   ],
   "id": "3838b11cd70573e1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:51.140137Z",
     "start_time": "2024-10-17T11:09:51.125111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainDf = pd.DataFrame({\"content\": train_documents})\n",
    "testDf = pd.DataFrame({\"content\": test_documents})"
   ],
   "id": "eeb28fb2b38c54aa",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4. Preprocesamiento de los documentos",
   "id": "67cae283f4c10dc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:51.469491Z",
     "start_time": "2024-10-17T11:09:51.141137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Cargar el lematizador y el stemmer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Cargar las stopwords desde un archivo\n",
    "stopwords = set(w.rstrip() for w in open('data/reuters/stopwords'))\n",
    "\n",
    "\n",
    "def tokenize_lemma_stopwords(text):\n",
    "    # Reemplazar saltos de línea por espacios\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Tokenizar el texto en palabras\n",
    "    tokens = nltk.tokenize.word_tokenize(text.lower())\n",
    "\n",
    "    # Filtrar tokens para mantener solo palabras alfabéticas\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "    # Lematizar las palabras para obtener su forma base\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    # Aplicar stemming a las palabras\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    # Eliminar palabras cortas (menos de 3 caracteres)\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "\n",
    "    # Unir los tokens en una cadena de texto limpia\n",
    "    cleanedText = \" \".join(tokens)\n",
    "\n",
    "    return cleanedText\n",
    "\n",
    "\n",
    "def dataCleaning(df):\n",
    "    # Crear una copia del DataFrame original\n",
    "    data = df.copy()\n",
    "\n",
    "    # Aplicar la función de limpieza a la columna 'content'\n",
    "    data[\"content\"] = data[\"content\"].apply(tokenize_lemma_stopwords)\n",
    "\n",
    "    return data"
   ],
   "id": "8be87ba4c0040df6",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/reuters/stopwords'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m stemmer \u001B[38;5;241m=\u001B[39m PorterStemmer()\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Cargar las stopwords desde un archivo\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m stopwords \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(w\u001B[38;5;241m.\u001B[39mrstrip() \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdata/reuters/stopwords\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize_lemma_stopwords\u001B[39m(text):\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# Reemplazar saltos de línea por espacios\u001B[39;00m\n\u001B[0;32m     14\u001B[0m     text \u001B[38;5;241m=\u001B[39m text\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Tesis\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    317\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    318\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    319\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    320\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    321\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    322\u001B[0m     )\n\u001B[1;32m--> 324\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/reuters/stopwords'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-17T11:09:51.470491Z",
     "start_time": "2024-10-17T11:09:51.470491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cleanedTrainData = dataCleaning(trainDf)\n",
    "cleanedTestData = dataCleaning(testDf)"
   ],
   "id": "72516ccbc57a73b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5. Vectorización de los documentos",
   "id": "9c89b7c0f780ce02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vectorised_train_documents = vectorizer.fit_transform(cleanedTrainData[\"content\"])\n",
    "vectorised_test_documents = vectorizer.transform(cleanedTestData[\"content\"])"
   ],
   "id": "6003b8bce6d3ae58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Suponiendo que `vectorizer` y `vectorised_train_documents` ya están definidos\n",
    "features = vectorizer.get_feature_names_out()\n",
    "frequencies = np.asarray(vectorised_train_documents.sum(axis=0)).flatten()\n",
    "\n",
    "# Ordenar las características por frecuencia\n",
    "sorted_indices = np.argsort(frequencies)[::-1]\n",
    "sorted_features = features[sorted_indices]\n",
    "sorted_frequencies = frequencies[sorted_indices]\n",
    "\n",
    "# Seleccionar las 50 características más frecuentes\n",
    "top_n = 50\n",
    "top_features = sorted_features[:top_n]\n",
    "top_frequencies = sorted_frequencies[:top_n]\n",
    "\n",
    "# Crear la gráfica\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_features, top_frequencies)\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Frequency Distribution of Top 50 tokens')\n",
    "plt.gca().invert_yaxis()  # Invertir el eje y para que la característica más frecuente esté en la parte superior\n",
    "plt.show()"
   ],
   "id": "5fc0d5404a6e0654",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "61de343a66ca52aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Implementación de modelos de clasificación",
   "id": "d6f71ae6e0f382e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1. Implementación las métricas de evaluación",
   "id": "deb2a617c2ad56fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ModelsPerformance = {}\n",
    "\n",
    "def metricsReport(modelName: str, test_labels, predictions) -> None:\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "    macro_precision = precision_score(test_labels, predictions, average='macro')\n",
    "    macro_recall = recall_score(test_labels, predictions, average='macro')\n",
    "    macro_f1 = f1_score(test_labels, predictions, average='macro')\n",
    "    hamLoss = hamming_loss(test_labels, predictions)\n",
    "    \n",
    "    print(f\"------{modelName} Model Metrics-----\")\n",
    "    print(\n",
    "        f\"Accuracy: {accuracy:.4f}\\nHamming Loss: {hamLoss:.4f}\\n\"\n",
    "        f\"Precision (Macro): {macro_precision:.4f}\\n\"\n",
    "        f\"Recall (Macro): {macro_recall:.4f}\\n\"\n",
    "        f\"F1-measure (Macro): {macro_f1:.4f}\"\n",
    "    )\n",
    "    ModelsPerformance[modelName] = macro_f1"
   ],
   "id": "ba4c9ec5911f2c8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2. Implementación de Naive Bayes",
   "id": "2e581fa4a2760c3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "naive_bayes_model = OneVsRestClassifier(MultinomialNB())\n",
    "naive_bayes_model.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "train_predictions = naive_bayes_model.predict(vectorised_train_documents)\n",
    "\n",
    "metricsReport(\"Naive Bayes\", train_labels, train_predictions)\n"
   ],
   "id": "76e59584e6d598ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3. Implementación de Regresión Logística Multinomial",
   "id": "2d4094a2d0c6b23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logistic_regression_model = OneVsRestClassifier(SGDClassifier(loss='log_loss', penalty='l2', alpha=1e-4, max_iter=1000, tol=1e-3))\n",
    "logistic_regression_model.fit(vectorised_train_documents, train_labels)\n",
    "\n",
    "train_predictions = logistic_regression_model.predict(vectorised_train_documents)\n",
    "\n",
    "metricsReport(\"Logistic Regression\", train_labels, train_predictions)"
   ],
   "id": "721bef48afebdf7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5d3ad07b6ba627b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
