{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "627da82214878c95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Examen Parcial CC0C2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332386881cebc7f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ejercicio 1\n",
    "\n",
    "Dadas tres oraciones \"all models are wrong\", \"a model is wrong\" y \"some models are useful\", y el vocabulario {< s >, < /s >, a, all, are, model, models, some, useful, wrong}. En código responde las siguientes preguntas "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67fabbf8c510353"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Calcule las probabilidades de todos los bigramas sin suavizado."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2068e66110023c8"
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['all', 'models', 'are', 'wrong', 'a', 'model', 'is', 'wrong', 'some', 'models', 'are', 'useful']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokeniza el texto y remueve signos de puntuación\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    return text.split()\n",
    "\n",
    "corpus = \"\"\"\n",
    "        all models are wrong\n",
    "        a model is wrong\n",
    "        some models are useful\n",
    "        \"\"\"\n",
    "\n",
    "tokens = tokenize(corpus)\n",
    "print(\"Tokens:\", tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T21:51:53.343385500Z",
     "start_time": "2024-10-19T21:51:53.331048400Z"
    }
   },
   "id": "abb54e34b49f3712"
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigramas: Counter({('models', 'are'): 2, ('all', 'models'): 1, ('are', 'wrong'): 1, ('wrong', 'a'): 1, ('a', 'model'): 1, ('model', 'is'): 1, ('is', 'wrong'): 1, ('wrong', 'some'): 1, ('some', 'models'): 1, ('are', 'useful'): 1})\n"
     ]
    }
   ],
   "source": [
    "# Clase2/Modelos-lenguaje1.ipynb\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_ngram_counts(tokens, n):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return Counter(ngrams)\n",
    "\n",
    "\n",
    "# Conteo de unigramas, bigramas y trigramas\n",
    "unigrams = build_ngram_counts(tokens, 1)\n",
    "bigrams = build_ngram_counts(tokens, 2)\n",
    "trigrams = build_ngram_counts(tokens, 3)\n",
    "\n",
    "print(\"Bigramas:\", bigrams)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T21:52:08.534070900Z",
     "start_time": "2024-10-19T21:52:08.524617100Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades de bigramas sin suavizado:\n",
      "\n",
      "P(models | all) = 1.0\n",
      "P(are | models) = 1.0\n",
      "P(wrong | are) = 0.5\n",
      "P(a | wrong) = 0.5\n",
      "P(model | a) = 1.0\n",
      "P(is | model) = 1.0\n",
      "P(wrong | is) = 1.0\n",
      "P(some | wrong) = 0.5\n",
      "P(models | some) = 1.0\n",
      "P(useful | are) = 0.5\n"
     ]
    }
   ],
   "source": [
    "# Clase2/Modelos-lenguaje1.ipynb\n",
    "def bigram_prob(bigrams, unigrams, word1, word2):\n",
    "    return bigrams[(word1, word2)] / unigrams[(word1,)]\n",
    "\n",
    "\n",
    "# @Overglitch\n",
    "# Respuesta de la pregunta\n",
    "print(\"Probabilidades de bigramas sin suavizado:\\n\")\n",
    "for bigram in bigrams:\n",
    "    word1, word2 = bigram\n",
    "    print(f\"P({word2} | {word1}) = {bigram_prob(bigrams, unigrams, word1, word2)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T21:52:10.394310900Z",
     "start_time": "2024-10-19T21:52:10.387642Z"
    }
   },
   "id": "7ee4cdb14a38e682"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Calcule las probabilidades de todos los bigramas y el bigrama no visto \"a models\" con suavizado de add-one."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "710d55f40e2fde9c"
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilidades de bigramas suavizadas con add-one:\n",
      "P(models | all) = 0.2000\n",
      "P(are | models) = 0.2727\n",
      "P(wrong | are) = 0.1818\n",
      "P(a | wrong) = 0.1818\n",
      "P(model | a) = 0.2000\n",
      "P(is | model) = 0.2000\n",
      "P(wrong | is) = 0.2000\n",
      "P(some | wrong) = 0.1818\n",
      "P(models | some) = 0.2000\n",
      "P(useful | are) = 0.1818\n",
      "P(models | a) = 0.0111 (bigrama no visto)\n"
     ]
    }
   ],
   "source": [
    "# Clase2/Modelos-lenguaje2.ipynb\n",
    "\n",
    "# Suavizado de Laplace para bigramas\n",
    "def laplace_smoothing_bigram(corpus, k=1):\n",
    "    # Conteo de bigramas y unigrams\n",
    "    bigram_counts = {}\n",
    "    unigram_counts = {}\n",
    "\n",
    "    # Construir bigramas\n",
    "    for i in range(len(corpus) - 1):\n",
    "        bigram = (corpus[i], corpus[i + 1])\n",
    "        unigram = corpus[i]\n",
    "\n",
    "        if bigram in bigram_counts:\n",
    "            bigram_counts[bigram] += 1\n",
    "        else:\n",
    "            bigram_counts[bigram] = 1\n",
    "\n",
    "        if unigram in unigram_counts:\n",
    "            unigram_counts[unigram] += 1\n",
    "        else:\n",
    "            unigram_counts[unigram] = 1\n",
    "\n",
    "    # Contar el último unigrama\n",
    "    last_word = corpus[-1]\n",
    "    if last_word in unigram_counts:\n",
    "        unigram_counts[last_word] += 1\n",
    "    else:\n",
    "        unigram_counts[last_word] = 1\n",
    "\n",
    "    # Tamaño del vocabulario\n",
    "    V = len(unigram_counts)\n",
    "\n",
    "    # Cálculo de las probabilidades suavizadas para bigramas\n",
    "    laplace_probabilities = {}\n",
    "    for bigram, bigram_count in bigram_counts.items():\n",
    "        w_n_1 = bigram[0]\n",
    "        # Aplicando la ecuación P_Laplace(w_n | w_n-1) = (C(w_n-1 w_n) + 1) / (C(w_n-1) + V)\n",
    "        # Aquí está el add-one smoothing\n",
    "        laplace_probabilities[bigram] = (bigram_count + k) / (unigram_counts[w_n_1] + k * V)\n",
    "\n",
    "    # Probabilidad para un bigrama no visto\n",
    "    laplace_probabilities[('a', 'models')] = k / (V * (V + k))\n",
    "\n",
    "    return laplace_probabilities\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = \"\"\"\n",
    "        all models are wrong\n",
    "        a model is wrong\n",
    "        some models are useful\n",
    "        \"\"\"\n",
    "corpus = tokenize(corpus)\n",
    "laplace_prob_bigrams = laplace_smoothing_bigram(corpus)\n",
    "print(\"\\nProbabilidades de bigramas suavizadas con add-one:\")\n",
    "\n",
    "# Imprimir las probabilidades de los bigramas\n",
    "for bigram, prob in laplace_prob_bigrams.items():\n",
    "    if bigram == ('a', 'models'):\n",
    "        print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f} (bigrama no visto)\")\n",
    "    else:\n",
    "        print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T21:52:12.880024200Z",
     "start_time": "2024-10-19T21:52:12.879511100Z"
    }
   },
   "id": "6a0e695129d915d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Calcule las probabilidades de todos los bigramas y el bigrama no visto \"a models\" con suavizado de add-k. Pruebe con k = 0.05 y k = 0.15."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eed35fb774a74a59"
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probabilidades de bigramas suavizadas con add-k (k = 0.05):\n",
      "P(models | all) = 0.7241\n",
      "P(are | models) = 0.8367\n",
      "P(wrong | are) = 0.4286\n",
      "P(a | wrong) = 0.4286\n",
      "P(model | a) = 0.7241\n",
      "P(is | model) = 0.7241\n",
      "P(wrong | is) = 0.7241\n",
      "P(some | wrong) = 0.4286\n",
      "P(models | some) = 0.7241\n",
      "P(useful | are) = 0.4286\n",
      "P(models | a) = 0.0006 (bigrama no visto)\n",
      "\n",
      "Probabilidades de bigramas suavizadas con add-k (k = 0.15):\n",
      "P(models | all) = 0.4894\n",
      "P(are | models) = 0.6418\n",
      "P(wrong | are) = 0.3433\n",
      "P(a | wrong) = 0.3433\n",
      "P(model | a) = 0.4894\n",
      "P(is | model) = 0.4894\n",
      "P(wrong | is) = 0.4894\n",
      "P(some | wrong) = 0.3433\n",
      "P(models | some) = 0.4894\n",
      "P(useful | are) = 0.3433\n",
      "P(models | a) = 0.0018 (bigrama no visto)\n"
     ]
    }
   ],
   "source": [
    "# Clase2/Modelos-lenguaje2.ipynb\n",
    "\n",
    "# Ejemplo de uso\n",
    "corpus = \"\"\"\n",
    "        all models are wrong\n",
    "        a model is wrong\n",
    "        some models are useful\n",
    "        \"\"\"\n",
    "corpus = tokenize(corpus)\n",
    "laplace_prob_bigrams = laplace_smoothing_bigram(corpus, k=0.05)\n",
    "print(\"\\nProbabilidades de bigramas suavizadas con add-k (k = 0.05):\")\n",
    "\n",
    "\n",
    "for bigram, prob in laplace_prob_bigrams.items():\n",
    "    if bigram == ('a', 'models'):\n",
    "        print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f} (bigrama no visto)\")\n",
    "    else:\n",
    "        print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
    "\n",
    "laplace_prob_bigrams = laplace_smoothing_bigram(corpus, k=0.15)\n",
    "print(\"\\nProbabilidades de bigramas suavizadas con add-k (k = 0.15):\")\n",
    "\n",
    "\n",
    "for bigram, prob in laplace_prob_bigrams.items():\n",
    "    if bigram == ('a', 'models'):\n",
    "        print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f} (bigrama no visto)\")\n",
    "    else:\n",
    "        print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T21:52:36.482992200Z",
     "start_time": "2024-10-19T21:52:36.481364100Z"
    }
   },
   "id": "24baf264c7012cb7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d) Calcule las probabilidades de todos los bigramas y el bigrama no visto \"a models\" con back-off y stupid-backoff."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a5c2466b715af56"
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidades de bigramas sin suavisado:\n",
      "\n",
      "P(all | <s>) = 1.0000\n",
      "P(models | all) = 1.0000\n",
      "P(are | models) = 1.0000\n",
      "P(wrong | are) = 0.5000\n",
      "P(a | wrong) = 0.5000\n",
      "P(model | a) = 1.0000\n",
      "P(is | model) = 1.0000\n",
      "P(wrong | is) = 1.0000\n",
      "P(some | wrong) = 0.5000\n",
      "P(models | some) = 1.0000\n",
      "P(useful | are) = 0.5000\n",
      "P(</s> | useful) = 1.0000\n",
      "Probabilidades del bigrama no visto 'a models':\n",
      "P(models | a) = 0.0000\n",
      "\n",
      "Probabilidades de bigramas con backoff:\n",
      "\n",
      "P(all | <s>) = 0.0769\n",
      "P(models | all) = 0.1538\n",
      "P(are | models) = 0.1538\n",
      "P(wrong | are) = 0.1538\n",
      "P(a | wrong) = 0.0769\n",
      "P(model | a) = 0.0769\n",
      "P(is | model) = 0.0769\n",
      "P(wrong | is) = 0.1538\n",
      "P(some | wrong) = 0.0769\n",
      "P(models | some) = 0.1538\n",
      "P(useful | are) = 0.0769\n",
      "P(</s> | useful) = 0.0769\n",
      "Probabilidades del bigrama no visto 'a models':\n",
      "P(models | a) = 0.1538\n",
      "\n",
      "Probabilidades de bigramas con stupid backoff:\n",
      "\n",
      "P(all | <s>) = 0.0769\n",
      "P(models | all) = 0.1538\n",
      "P(are | models) = 0.1538\n",
      "P(wrong | are) = 0.1538\n",
      "P(a | wrong) = 0.0769\n",
      "P(model | a) = 0.0769\n",
      "P(is | model) = 0.0769\n",
      "P(wrong | is) = 0.1538\n",
      "P(some | wrong) = 0.0769\n",
      "P(models | some) = 0.1538\n",
      "P(useful | are) = 0.0769\n",
      "P(</s> | useful) = 0.0769\n",
      "Probabilidades del bigrama no visto 'a models':\n",
      "P(models | a) = 0.1538\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "# Implementación del NGramModel\n",
    "class NGramModel:\n",
    "    def __init__(self, n: int):\n",
    "        self.n = n\n",
    "        self.ngram_counts = collections.Counter()\n",
    "        self.context_counts = collections.Counter()\n",
    "        self.vocab = set()\n",
    "        self.total_ngrams = 0\n",
    "\n",
    "    def train(self, corpus: List[List[str]]):\n",
    "        for document in corpus:\n",
    "            tokens = ['<s>'] * (self.n - 1) + document + ['</s>']\n",
    "            self.vocab.update(tokens)\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                ngram = tuple(tokens[i:i + self.n])\n",
    "                context = tuple(tokens[i:i + self.n - 1])\n",
    "                self.ngram_counts[ngram] += 1\n",
    "                self.context_counts[context] += 1\n",
    "                self.total_ngrams += 1\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        count = self.ngram_counts.get(ngram, 0)\n",
    "        context = ngram[:-1]\n",
    "        context_count = self.context_counts.get(context, 0)\n",
    "        if context_count == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return count / context_count\n",
    "\n",
    "    def get_sentence_probability(self, sentence: List[str]) -> float:\n",
    "        tokens = ['<s>'] * (self.n - 1) + sentence + ['</s>']\n",
    "        probability = 1.0\n",
    "        for i in range(len(tokens) - self.n + 1):\n",
    "            ngram = tuple(tokens[i:i + self.n])\n",
    "            prob = self.get_ngram_prob(ngram)\n",
    "            if prob > 0:\n",
    "                probability *= prob\n",
    "            else:\n",
    "                # Asignamos una pequeña probabilidad para evitar cero\n",
    "                probability *= 1e-6\n",
    "        return probability\n",
    "\n",
    "\n",
    "# Implementación de Backoff Estándar\n",
    "\n",
    "class BackoffNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, models: List[NGramModel]):\n",
    "        super().__init__(n)\n",
    "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
    "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
    "        self.vocab = set()\n",
    "        for model in self.models:\n",
    "            self.vocab.update(model.vocab)\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        for model in self.models:\n",
    "            ngram_adjusted = ngram[-model.n:]\n",
    "            prob = model.get_ngram_prob(ngram_adjusted)\n",
    "            if prob > 0:\n",
    "                return prob\n",
    "        # Si ningún modelo tiene el n-grama, asignamos una pequeña probabilidad\n",
    "        return 1e-6\n",
    "\n",
    "\n",
    "# Implementación del Stupid Backoff\n",
    "\n",
    "class StupidBackoffNGramModel(NGramModel):\n",
    "    def __init__(self, n: int, models: List[NGramModel], alpha: float = 0.4):\n",
    "        super().__init__(n)\n",
    "        self.models = models  # Lista de modelos de diferentes órdenes, ordenados de mayor a menor\n",
    "        self.alpha = alpha  # Factor de escala fijo\n",
    "        # Actualizamos self.vocab con la unión de los vocabularios de los modelos\n",
    "        self.vocab = set()\n",
    "        for model in self.models:\n",
    "            self.vocab.update(model.vocab)\n",
    "\n",
    "    def get_ngram_prob(self, ngram: Tuple[str, ...]) -> float:\n",
    "        for i, model in enumerate(self.models):\n",
    "            ngram_adjusted = ngram[-model.n:]\n",
    "            prob = model.get_ngram_prob(ngram_adjusted)\n",
    "            if prob > 0:\n",
    "                return (self.alpha ** i) * prob\n",
    "        # Si ningún modelo tiene el n-grama, asignamos una pequeña probabilidad\n",
    "        return (self.alpha ** len(self.models)) * (1.0 / len(self.vocab))\n",
    "\n",
    "\n",
    "# Entrenamiento de los modelos\n",
    "corpus = \"\"\"\n",
    "        all models are wrong\n",
    "        a model is wrong\n",
    "        some models are useful\n",
    "        \"\"\"\n",
    "\n",
    "train_corpus = tokenize(corpus)\n",
    "vocab = ['<s>', '</s>', 'a', 'all', 'are', 'model', 'models', 'some', 'useful', 'wrong']\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "unigram_model = NGramModel(1)\n",
    "bigram_model = NGramModel(2)\n",
    "trigram_model = NGramModel(3)\n",
    "\n",
    "for model in [unigram_model, bigram_model, trigram_model]:\n",
    "    model.train([train_corpus])\n",
    "\n",
    "#muestra todas las probabilidades de los bigramas\n",
    "print(\"Probabilidades de bigramas sin suavisado:\\n\")\n",
    "for bigram in bigram_model.ngram_counts:\n",
    "    prob = bigram_model.get_ngram_prob(bigram)\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
    "print(\"Probabilidades del bigrama no visto 'a models':\")\n",
    "print(f\"P(models | a) = {bigram_model.get_ngram_prob(('a', 'models')):.4f}\")\n",
    "\n",
    "#muestra todas las probabilidades de los bigramas con backoff\n",
    "print(\"\\nProbabilidades de bigramas con backoff:\\n\")\n",
    "backoff_model = BackoffNGramModel(2, [unigram_model, bigram_model])\n",
    "for bigram in bigram_model.ngram_counts:\n",
    "    prob = backoff_model.get_ngram_prob(bigram)\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
    "print(\"Probabilidades del bigrama no visto 'a models':\")\n",
    "print(f\"P(models | a) = {backoff_model.get_ngram_prob(('a', 'models')):.4f}\")\n",
    "\n",
    "#muestra todas las probabilidades de los bigramas con stupid backoff\n",
    "print(\"\\nProbabilidades de bigramas con stupid backoff:\\n\")\n",
    "stupid_backoff_model = StupidBackoffNGramModel(2, [unigram_model, bigram_model])\n",
    "for bigram in bigram_model.ngram_counts:\n",
    "    prob = stupid_backoff_model.get_ngram_prob(bigram)\n",
    "    print(f\"P({bigram[1]} | {bigram[0]}) = {prob:.4f}\")\n",
    "print(\"Probabilidades del bigrama no visto 'a models':\")\n",
    "print(f\"P(models | a) = {stupid_backoff_model.get_ngram_prob(('a', 'models')):.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-19T22:02:27.490837100Z",
     "start_time": "2024-10-19T22:02:27.485852300Z"
    }
   },
   "id": "bf6fee8147dc4343"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
